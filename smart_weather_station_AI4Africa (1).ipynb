{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "gav8Jgh12xK_"
      },
      "outputs": [],
      "source": [
        "#iimports\n",
        "import glob\n",
        "from math import e, gamma\n",
        "import os\n",
        "from pathlib import Path\n",
        "from turtle import color\n",
        "from librosa import feature\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "# import scikitplot as skplt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from joblib import dump, load\n",
        "from statistics import mean\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERUgkdJGba4g",
        "outputId": "88f91c75-e63e-4410-96ce-57ae27bbae9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (0.1.7)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahrYrX_3aZ66",
        "outputId": "0c12bc71-9ee3-402f-ee07-52084061afcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZusYefr2xLA"
      },
      "source": [
        "SPLITTING THE AUDIOFILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hET_N6Z62xLB"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "audiorec=[]\n",
        "count=0\n",
        "def split(filez):\n",
        "    \n",
        "    split_into = 5\n",
        "    samples_splitted = []\n",
        "    current_step = 0\n",
        "    i=0\n",
        "    X,sample_rate = librosa.load(filez, res_type=\"kaiser_best\" )\n",
        "    while current_step < len(X):\n",
        "        samples_splitted.append(X[current_step: split_into * sample_rate ])\n",
        "        sf.write(f\"set22-{i} outfile.wav\", samples_splitted[i], sample_rate)\n",
        "        current_step = split_into * sample_rate\n",
        "        i+=1\n",
        "\n",
        "        split_into += 5\n",
        "    \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqMVzjRz2xLB"
      },
      "outputs": [],
      "source": [
        "split('audiofile')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnMijQkw2xLC"
      },
      "source": [
        "CREATING LIST FOR SPLIT AUDIOFILE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5oaXKLK2xLC"
      },
      "outputs": [],
      "source": [
        "path= 'splitaudio/'\n",
        "files = os.listdir(path)\n",
        "audiofile = []\n",
        "for filename in glob.glob(os.path.join(path, '*.wav')):\n",
        "    data, samplerate = librosa.load(filename, res_type='kaiser_best')\n",
        "    audiofile.append(data)\n",
        "\n",
        "print(len(audiofile))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNkbcSlk2xLC",
        "outputId": "1b90135c-f7b4-45fb-b427-c7247b2add6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1595\n"
          ]
        }
      ],
      "source": [
        "NO_PER_LABEL =NO_PER_LABEL =[65, 60, 60, 60, 61, 60, 60, 60, 60, 61, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 60, 61, 60, 60]\n",
        "readings = [0.1, 2.5, 4.6, 6.6, 5.5, 5, 3.2, 0.5, 0.1, 1.5, 0.3, 2, 3.5, 3.5, 1.5, 3.5, 5.5, 5.5, 1.5, 1, 1, 0.4, 0.09, 1.8, 0.5, 0.08]\n",
        "label_list=[]\n",
        "for i,reading in zip(NO_PER_LABEL, readings):\n",
        "    count=0\n",
        "    while count <= i:\n",
        "        label_list.append(reading)\n",
        "        count+=1\n",
        "print(len(label_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "j_cUv8Tv2xLD"
      },
      "outputs": [],
      "source": [
        "def extract_features(files):\n",
        "    \n",
        "    # Sets the name to be the path to where the file is in my computer\n",
        "    #file_name = os.path.join(os.path.abspath('voice')+'/'+str(files.file))\n",
        "    #file_name = files.audio_files\n",
        "    # Loads the audio file as a floating point time series and assigns the default sample rate\n",
        "    # Sample rate is set to 22050 by default\n",
        "    #X, sample_rate = librosa.load(file_name, res_type='kaiser_best')     \n",
        "\n",
        "    # Generate Mel-frequency cepstral coefficients (MFCCs) from a time series \n",
        "    mfccs = np.mean(librosa.feature.mfcc(y=files, sr=22050, n_mfcc=12).T,axis=0) ## DIGEST MORE ON THE MFCC\n",
        "    # mfccs = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=40))\n",
        "    # mfccs = librosa.feature.chroma_stft(y=X, sr=sample_rate)\n",
        "    # mfccs = librosa.lpc(y=X, order=2)\n",
        "    \n",
        "    # We add also the classes of each file as a label at the end\n",
        "    #label = files.labels\n",
        "\n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0mT3phM2xLD"
      },
      "outputs": [],
      "source": [
        "mfcc_list = []\n",
        "for file in audiofile:\n",
        "    mfcc_list.append(extract_features(file))\n",
        "print(len(mfcc_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dfNfqAK2xLD"
      },
      "outputs": [],
      "source": [
        "np.savez_compressed(\"rain_dataset\", mfcc=mfcc_list, labels=label_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "dWVmwtrE2xLD"
      },
      "outputs": [],
      "source": [
        "loaded = np.load('/content/drive/MyDrive/rain_dataset.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGzvQCaO2xLD",
        "outputId": "c4a51230-9f58-4226-9d10-7840c79d4564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1595\n"
          ]
        }
      ],
      "source": [
        "print(len(loaded['mfcc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f_CcVdC-2xLE"
      },
      "outputs": [],
      "source": [
        "X = loaded[\"mfcc\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "otCaOJwD2xLE"
      },
      "outputs": [],
      "source": [
        "X= X.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y0CJP-02xLE",
        "outputId": "3b4682cd-5295-4903-db35-e6a3181a562c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19140"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Otz6-kzU2xLE"
      },
      "outputs": [],
      "source": [
        "\n",
        "from itertools import islice\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Hr47-1uM2xLE"
      },
      "outputs": [],
      "source": [
        "me = chunk(X, 288)\n",
        "label_list = chunk(label_list, 24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8k2LhexH2xLE"
      },
      "outputs": [],
      "source": [
        "me = list(me)\n",
        "label_list = list(label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5634n_ji2xLE"
      },
      "outputs": [],
      "source": [
        "new_labels = []\n",
        "for val in label_list:\n",
        "    new_labels.append(val[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9QbG6eSc2xLE"
      },
      "outputs": [],
      "source": [
        "data = np.array(me, dtype=object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r_2oTyFk2xLE"
      },
      "outputs": [],
      "source": [
        "newme =[]\n",
        "for dat in me[:-1]:\n",
        "    newme.append(np.array(dat, dtype=np.float64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bWLx8qyM2xLF"
      },
      "outputs": [],
      "source": [
        "newme = np.array(newme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0qhw6Fj2xLF",
        "outputId": "1dc6cb71-0f4c-4f21-e955-89f6656c24bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(newme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiC_g7xn2xLF",
        "outputId": "b9ced425-ea82-4ca5-d12c-6ef9846bb024"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(66, 288)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "newme.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoAcNXBP2xLF",
        "outputId": "1057c15c-486d-422b-cc8b-de350249c1d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "len(new_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R5hJA-882xLF"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "# good_list_scaled = (scaler.fit_transform(good_list))\n",
        "good_list_scaled = (scaler.fit_transform(newme))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fW1NmYyZBaQ"
      },
      "source": [
        "TRAINING THE MODEL USING SVR(POLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q04PWvEB2xLF",
        "outputId": "16d4b736-3f5a-4022-e58b-1f81d8880810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LEN OF DATA -> 319\n",
            " The Mean absolute percentage Error is -> 138.7073198432046 %\n"
          ]
        }
      ],
      "source": [
        "## try a Deep Neural Network\n",
        "X = np.array(loaded['mfcc'])\n",
        "#X = good_list_scaled\n",
        "Y = label_list\n",
        "#Y =  new_labels[:-1]\n",
        "# Splitting dataset into train and test set\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "X, Y, test_size = 1/5, random_state = 0, shuffle=True)\n",
        "model = SVR(kernel=\"poly\")\n",
        "# model = RandomForestRegressor(max_depth=None, random_state=0)\n",
        "model.fit( X_train, Y_train )\n",
        "\n",
        "# Prediction on test set\n",
        "print(\"LEN OF DATA ->\",len(X_test))\n",
        "Y_pred = model.predict( X_test )\n",
        "\n",
        "#print( \"Predicted values \", np.round( Y_pred, 2 ) )\n",
        "\n",
        "#print( \"Real values\t \", Y_test[:30] )\n",
        "\n",
        "#print( \" Coefficients of the LR \",  model.coef_, model.intercept_ )\n",
        "\n",
        "\n",
        "\n",
        "mse = mean_absolute_percentage_error(Y_test, Y_pred) * 100 ##( |ypred - ytrue|/ytrue * 100)\n",
        "\n",
        "print(' The Mean absolute percentage Error is ->', mse, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uv2NRGr2xLG"
      },
      "source": [
        "TRAINING THE MODEL USING XGB-REGRESSOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pkrfgsr2xLG",
        "outputId": "2eeec50a-5a68-41bd-89a4-8685f3905bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values  [2.55 2.09 2.  ]\n",
            "Real values\t  [5.5, 0.3, 2]\n",
            " The Mean absolute percentage Error is -> 106.69729539722607 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Instantiate gb\n",
        "gb = GradientBoostingRegressor(max_depth=4,\n",
        "                               n_estimators=200,\n",
        "                               random_state=2)\n",
        "gb.fit( X_train, Y_train )\n",
        "\n",
        "# Predict test set labels\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "print( \"Predicted values \", np.round( Y_pred[:3], 2 ) )\n",
        "\n",
        "print( \"Real values\t \", Y_test[:3] )\n",
        "\n",
        "#print( \" Coefficients of the LR \",  model.coef_, model.intercept_ )\n",
        "\n",
        "\n",
        "\n",
        "mse = mean_absolute_percentage_error(Y_test, Y_pred) * 100 ##( |ypred - ytrue|/ytrue * 100)\n",
        "\n",
        "print(' The Mean absolute percentage Error is ->', mse, '%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICghfOp2xLG"
      },
      "source": [
        "MODEL TRAINING USING DEEP NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "MkuQpZyj2xLG"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D, Conv2D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "import tensorflow as tf\n",
        "from keras.layers import Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "9UJMps7T2xLG"
      },
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    return (K.mean((K.abs(y_pred - y_true))/y_true) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1Lixl-j2xLG",
        "outputId": "58b2b4f4-b138-4689-fe1f-95e110d4bd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_36 (Dense)            (None, 6)                 78        \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 3)                 21        \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 103\n",
            "Trainable params: 103\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# dnn_neural = Sequential([\n",
        "#     tf.keras.layers.Input(shape = X_train.shape[1:]),\n",
        "#     # Dense(10, activation='softsign'),\n",
        "#     # Dense(6, activation='softsign'),\n",
        "#     Dense(3, activation='softsign'),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "NN_model = Sequential()\n",
        "NN_model.add(Dense(6, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "# NN_model.add(Dense(6, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(3, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMzskrcG2xLG",
        "outputId": "788c5da1-7139-465a-bc0a-ab2705c6854d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "32/32 [==============================] - 1s 6ms/step - loss: 2.2009 - mean_absolute_error: 2.2009 - val_loss: 2.2426 - val_mean_absolute_error: 2.2426\n",
            "Epoch 2/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.9936 - mean_absolute_error: 1.9936 - val_loss: 1.9290 - val_mean_absolute_error: 1.9290\n",
            "Epoch 3/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.7006 - mean_absolute_error: 1.7006 - val_loss: 1.4746 - val_mean_absolute_error: 1.4746\n",
            "Epoch 4/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.2017 - mean_absolute_error: 1.2017 - val_loss: 1.0051 - val_mean_absolute_error: 1.0051\n",
            "Epoch 5/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8541 - mean_absolute_error: 0.8541 - val_loss: 0.8548 - val_mean_absolute_error: 0.8548\n",
            "Epoch 6/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7473 - mean_absolute_error: 0.7473 - val_loss: 0.8452 - val_mean_absolute_error: 0.8452\n",
            "Epoch 7/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7217 - mean_absolute_error: 0.7217 - val_loss: 0.8322 - val_mean_absolute_error: 0.8322\n",
            "Epoch 8/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7154 - mean_absolute_error: 0.7154 - val_loss: 0.8095 - val_mean_absolute_error: 0.8095\n",
            "Epoch 9/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6875 - mean_absolute_error: 0.6875 - val_loss: 0.7904 - val_mean_absolute_error: 0.7904\n",
            "Epoch 10/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6717 - mean_absolute_error: 0.6717 - val_loss: 0.7750 - val_mean_absolute_error: 0.7750\n",
            "Epoch 11/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6620 - mean_absolute_error: 0.6620 - val_loss: 0.7563 - val_mean_absolute_error: 0.7563\n",
            "Epoch 12/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6455 - mean_absolute_error: 0.6455 - val_loss: 0.7397 - val_mean_absolute_error: 0.7397\n",
            "Epoch 13/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6269 - mean_absolute_error: 0.6269 - val_loss: 0.7346 - val_mean_absolute_error: 0.7346\n",
            "Epoch 14/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6164 - mean_absolute_error: 0.6164 - val_loss: 0.7187 - val_mean_absolute_error: 0.7187\n",
            "Epoch 15/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.6119 - mean_absolute_error: 0.6119 - val_loss: 0.6967 - val_mean_absolute_error: 0.6967\n",
            "Epoch 16/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5913 - mean_absolute_error: 0.5913 - val_loss: 0.6861 - val_mean_absolute_error: 0.6861\n",
            "Epoch 17/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5854 - mean_absolute_error: 0.5854 - val_loss: 0.6690 - val_mean_absolute_error: 0.6690\n",
            "Epoch 18/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5801 - mean_absolute_error: 0.5801 - val_loss: 0.6694 - val_mean_absolute_error: 0.6694\n",
            "Epoch 19/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5733 - mean_absolute_error: 0.5733 - val_loss: 0.6555 - val_mean_absolute_error: 0.6555\n",
            "Epoch 20/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5732 - mean_absolute_error: 0.5732 - val_loss: 0.6666 - val_mean_absolute_error: 0.6666\n",
            "Epoch 21/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5683 - mean_absolute_error: 0.5683 - val_loss: 0.6493 - val_mean_absolute_error: 0.6493\n",
            "Epoch 22/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5672 - mean_absolute_error: 0.5672 - val_loss: 0.6497 - val_mean_absolute_error: 0.6497\n",
            "Epoch 23/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5743 - mean_absolute_error: 0.5743 - val_loss: 0.6514 - val_mean_absolute_error: 0.6514\n",
            "Epoch 24/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5645 - mean_absolute_error: 0.5645 - val_loss: 0.6411 - val_mean_absolute_error: 0.6411\n",
            "Epoch 25/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5611 - mean_absolute_error: 0.5611 - val_loss: 0.6791 - val_mean_absolute_error: 0.6791\n",
            "Epoch 26/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5697 - mean_absolute_error: 0.5697 - val_loss: 0.6431 - val_mean_absolute_error: 0.6431\n",
            "Epoch 27/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5600 - mean_absolute_error: 0.5600 - val_loss: 0.6363 - val_mean_absolute_error: 0.6363\n",
            "Epoch 28/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5572 - mean_absolute_error: 0.5572 - val_loss: 0.6464 - val_mean_absolute_error: 0.6464\n",
            "Epoch 29/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5607 - mean_absolute_error: 0.5607 - val_loss: 0.6405 - val_mean_absolute_error: 0.6405\n",
            "Epoch 30/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5573 - mean_absolute_error: 0.5573 - val_loss: 0.6325 - val_mean_absolute_error: 0.6325\n",
            "Epoch 31/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5563 - mean_absolute_error: 0.5563 - val_loss: 0.6320 - val_mean_absolute_error: 0.6320\n",
            "Epoch 32/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5548 - mean_absolute_error: 0.5548 - val_loss: 0.6349 - val_mean_absolute_error: 0.6349\n",
            "Epoch 33/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5553 - mean_absolute_error: 0.5553 - val_loss: 0.6346 - val_mean_absolute_error: 0.6346\n",
            "Epoch 34/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5538 - mean_absolute_error: 0.5538 - val_loss: 0.6463 - val_mean_absolute_error: 0.6463\n",
            "Epoch 35/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5606 - mean_absolute_error: 0.5606 - val_loss: 0.6385 - val_mean_absolute_error: 0.6385\n",
            "Epoch 36/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5541 - mean_absolute_error: 0.5541 - val_loss: 0.6324 - val_mean_absolute_error: 0.6324\n",
            "Epoch 37/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5568 - mean_absolute_error: 0.5568 - val_loss: 0.6321 - val_mean_absolute_error: 0.6321\n",
            "Epoch 38/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5549 - mean_absolute_error: 0.5549 - val_loss: 0.6316 - val_mean_absolute_error: 0.6316\n",
            "Epoch 39/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5563 - mean_absolute_error: 0.5563 - val_loss: 0.6370 - val_mean_absolute_error: 0.6370\n",
            "Epoch 40/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5517 - mean_absolute_error: 0.5517 - val_loss: 0.6276 - val_mean_absolute_error: 0.6276\n",
            "Epoch 41/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5507 - mean_absolute_error: 0.5507 - val_loss: 0.6491 - val_mean_absolute_error: 0.6491\n",
            "Epoch 42/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5535 - mean_absolute_error: 0.5535 - val_loss: 0.6258 - val_mean_absolute_error: 0.6258\n",
            "Epoch 43/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5517 - mean_absolute_error: 0.5517 - val_loss: 0.6305 - val_mean_absolute_error: 0.6305\n",
            "Epoch 44/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5529 - mean_absolute_error: 0.5529 - val_loss: 0.6266 - val_mean_absolute_error: 0.6266\n",
            "Epoch 45/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5517 - mean_absolute_error: 0.5517 - val_loss: 0.6233 - val_mean_absolute_error: 0.6233\n",
            "Epoch 46/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5487 - mean_absolute_error: 0.5487 - val_loss: 0.6446 - val_mean_absolute_error: 0.6446\n",
            "Epoch 47/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5547 - mean_absolute_error: 0.5547 - val_loss: 0.6210 - val_mean_absolute_error: 0.6210\n",
            "Epoch 48/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5488 - mean_absolute_error: 0.5488 - val_loss: 0.6533 - val_mean_absolute_error: 0.6533\n",
            "Epoch 49/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5490 - mean_absolute_error: 0.5490 - val_loss: 0.6318 - val_mean_absolute_error: 0.6318\n",
            "Epoch 50/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5430 - mean_absolute_error: 0.5430 - val_loss: 0.6220 - val_mean_absolute_error: 0.6220\n",
            "Epoch 51/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5457 - mean_absolute_error: 0.5457 - val_loss: 0.6253 - val_mean_absolute_error: 0.6253\n",
            "Epoch 52/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5481 - mean_absolute_error: 0.5481 - val_loss: 0.6246 - val_mean_absolute_error: 0.6246\n",
            "Epoch 53/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5456 - mean_absolute_error: 0.5456 - val_loss: 0.6191 - val_mean_absolute_error: 0.6191\n",
            "Epoch 54/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5541 - mean_absolute_error: 0.5541 - val_loss: 0.6242 - val_mean_absolute_error: 0.6242\n",
            "Epoch 55/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5448 - mean_absolute_error: 0.5448 - val_loss: 0.6208 - val_mean_absolute_error: 0.6208\n",
            "Epoch 56/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5611 - mean_absolute_error: 0.5611 - val_loss: 0.6174 - val_mean_absolute_error: 0.6174\n",
            "Epoch 57/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5449 - mean_absolute_error: 0.5449 - val_loss: 0.6254 - val_mean_absolute_error: 0.6254\n",
            "Epoch 58/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5408 - mean_absolute_error: 0.5408 - val_loss: 0.6175 - val_mean_absolute_error: 0.6175\n",
            "Epoch 59/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5443 - mean_absolute_error: 0.5443 - val_loss: 0.6176 - val_mean_absolute_error: 0.6176\n",
            "Epoch 60/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5477 - mean_absolute_error: 0.5477 - val_loss: 0.6195 - val_mean_absolute_error: 0.6195\n",
            "Epoch 61/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5447 - mean_absolute_error: 0.5447 - val_loss: 0.6485 - val_mean_absolute_error: 0.6485\n",
            "Epoch 62/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5476 - mean_absolute_error: 0.5476 - val_loss: 0.6185 - val_mean_absolute_error: 0.6185\n",
            "Epoch 63/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5429 - mean_absolute_error: 0.5429 - val_loss: 0.6186 - val_mean_absolute_error: 0.6186\n",
            "Epoch 64/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5405 - mean_absolute_error: 0.5405 - val_loss: 0.6197 - val_mean_absolute_error: 0.6197\n",
            "Epoch 65/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5461 - mean_absolute_error: 0.5461 - val_loss: 0.6152 - val_mean_absolute_error: 0.6152\n",
            "Epoch 66/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5473 - mean_absolute_error: 0.5473 - val_loss: 0.6239 - val_mean_absolute_error: 0.6239\n",
            "Epoch 67/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5458 - mean_absolute_error: 0.5458 - val_loss: 0.6288 - val_mean_absolute_error: 0.6288\n",
            "Epoch 68/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5421 - mean_absolute_error: 0.5421 - val_loss: 0.6236 - val_mean_absolute_error: 0.6236\n",
            "Epoch 69/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5443 - mean_absolute_error: 0.5443 - val_loss: 0.6142 - val_mean_absolute_error: 0.6142\n",
            "Epoch 70/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5386 - mean_absolute_error: 0.5386 - val_loss: 0.6145 - val_mean_absolute_error: 0.6145\n",
            "Epoch 71/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5443 - mean_absolute_error: 0.5443 - val_loss: 0.6166 - val_mean_absolute_error: 0.6166\n",
            "Epoch 72/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5382 - mean_absolute_error: 0.5382 - val_loss: 0.6162 - val_mean_absolute_error: 0.6162\n",
            "Epoch 73/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5365 - mean_absolute_error: 0.5365 - val_loss: 0.6214 - val_mean_absolute_error: 0.6214\n",
            "Epoch 74/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5403 - mean_absolute_error: 0.5403 - val_loss: 0.6613 - val_mean_absolute_error: 0.6613\n",
            "Epoch 75/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5452 - mean_absolute_error: 0.5452 - val_loss: 0.6117 - val_mean_absolute_error: 0.6117\n",
            "Epoch 76/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5431 - mean_absolute_error: 0.5431 - val_loss: 0.6265 - val_mean_absolute_error: 0.6265\n",
            "Epoch 77/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5370 - mean_absolute_error: 0.5370 - val_loss: 0.6112 - val_mean_absolute_error: 0.6112\n",
            "Epoch 78/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5408 - mean_absolute_error: 0.5408 - val_loss: 0.6171 - val_mean_absolute_error: 0.6171\n",
            "Epoch 79/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5376 - mean_absolute_error: 0.5376 - val_loss: 0.6135 - val_mean_absolute_error: 0.6135\n",
            "Epoch 80/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5345 - mean_absolute_error: 0.5345 - val_loss: 0.6228 - val_mean_absolute_error: 0.6228\n",
            "Epoch 81/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5401 - mean_absolute_error: 0.5401 - val_loss: 0.6142 - val_mean_absolute_error: 0.6142\n",
            "Epoch 82/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5360 - mean_absolute_error: 0.5360 - val_loss: 0.6095 - val_mean_absolute_error: 0.6095\n",
            "Epoch 83/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5386 - mean_absolute_error: 0.5386 - val_loss: 0.6133 - val_mean_absolute_error: 0.6133\n",
            "Epoch 84/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5349 - mean_absolute_error: 0.5349 - val_loss: 0.6101 - val_mean_absolute_error: 0.6101\n",
            "Epoch 85/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5387 - mean_absolute_error: 0.5387 - val_loss: 0.6246 - val_mean_absolute_error: 0.6246\n",
            "Epoch 86/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5367 - mean_absolute_error: 0.5367 - val_loss: 0.6126 - val_mean_absolute_error: 0.6126\n",
            "Epoch 87/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5394 - mean_absolute_error: 0.5394 - val_loss: 0.6107 - val_mean_absolute_error: 0.6107\n",
            "Epoch 88/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5422 - mean_absolute_error: 0.5422 - val_loss: 0.6141 - val_mean_absolute_error: 0.6141\n",
            "Epoch 89/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5333 - mean_absolute_error: 0.5333 - val_loss: 0.6083 - val_mean_absolute_error: 0.6083\n",
            "Epoch 90/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5323 - mean_absolute_error: 0.5323 - val_loss: 0.6125 - val_mean_absolute_error: 0.6125\n",
            "Epoch 91/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5310 - mean_absolute_error: 0.5310 - val_loss: 0.6082 - val_mean_absolute_error: 0.6082\n",
            "Epoch 92/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5337 - mean_absolute_error: 0.5337 - val_loss: 0.6092 - val_mean_absolute_error: 0.6092\n",
            "Epoch 93/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5335 - mean_absolute_error: 0.5335 - val_loss: 0.6074 - val_mean_absolute_error: 0.6074\n",
            "Epoch 94/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5333 - mean_absolute_error: 0.5333 - val_loss: 0.6066 - val_mean_absolute_error: 0.6066\n",
            "Epoch 95/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5337 - mean_absolute_error: 0.5337 - val_loss: 0.6154 - val_mean_absolute_error: 0.6154\n",
            "Epoch 96/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5312 - mean_absolute_error: 0.5312 - val_loss: 0.6070 - val_mean_absolute_error: 0.6070\n",
            "Epoch 97/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5477 - mean_absolute_error: 0.5477 - val_loss: 0.6215 - val_mean_absolute_error: 0.6215\n",
            "Epoch 98/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5440 - mean_absolute_error: 0.5440 - val_loss: 0.6153 - val_mean_absolute_error: 0.6153\n",
            "Epoch 99/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5316 - mean_absolute_error: 0.5316 - val_loss: 0.6097 - val_mean_absolute_error: 0.6097\n",
            "Epoch 100/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5318 - mean_absolute_error: 0.5318 - val_loss: 0.6100 - val_mean_absolute_error: 0.6100\n",
            "Epoch 101/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5361 - mean_absolute_error: 0.5361 - val_loss: 0.6102 - val_mean_absolute_error: 0.6102\n",
            "Epoch 102/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5292 - mean_absolute_error: 0.5292 - val_loss: 0.6048 - val_mean_absolute_error: 0.6048\n",
            "Epoch 103/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5311 - mean_absolute_error: 0.5311 - val_loss: 0.6053 - val_mean_absolute_error: 0.6053\n",
            "Epoch 104/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5318 - mean_absolute_error: 0.5318 - val_loss: 0.6044 - val_mean_absolute_error: 0.6044\n",
            "Epoch 105/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5291 - mean_absolute_error: 0.5291 - val_loss: 0.6041 - val_mean_absolute_error: 0.6041\n",
            "Epoch 106/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5316 - mean_absolute_error: 0.5316 - val_loss: 0.6068 - val_mean_absolute_error: 0.6068\n",
            "Epoch 107/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5290 - mean_absolute_error: 0.5290 - val_loss: 0.6068 - val_mean_absolute_error: 0.6068\n",
            "Epoch 108/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5285 - mean_absolute_error: 0.5285 - val_loss: 0.6208 - val_mean_absolute_error: 0.6208\n",
            "Epoch 109/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5319 - mean_absolute_error: 0.5319 - val_loss: 0.6056 - val_mean_absolute_error: 0.6056\n",
            "Epoch 110/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5281 - mean_absolute_error: 0.5281 - val_loss: 0.6030 - val_mean_absolute_error: 0.6030\n",
            "Epoch 111/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5310 - mean_absolute_error: 0.5310 - val_loss: 0.6047 - val_mean_absolute_error: 0.6047\n",
            "Epoch 112/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5298 - mean_absolute_error: 0.5298 - val_loss: 0.6028 - val_mean_absolute_error: 0.6028\n",
            "Epoch 113/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5281 - mean_absolute_error: 0.5281 - val_loss: 0.6115 - val_mean_absolute_error: 0.6115\n",
            "Epoch 114/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5307 - mean_absolute_error: 0.5307 - val_loss: 0.6033 - val_mean_absolute_error: 0.6033\n",
            "Epoch 115/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_absolute_error: 0.5256 - val_loss: 0.6032 - val_mean_absolute_error: 0.6032\n",
            "Epoch 116/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_absolute_error: 0.5258 - val_loss: 0.6030 - val_mean_absolute_error: 0.6030\n",
            "Epoch 117/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5270 - mean_absolute_error: 0.5270 - val_loss: 0.6022 - val_mean_absolute_error: 0.6022\n",
            "Epoch 118/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5243 - mean_absolute_error: 0.5243 - val_loss: 0.6042 - val_mean_absolute_error: 0.6042\n",
            "Epoch 119/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_absolute_error: 0.5255 - val_loss: 0.6194 - val_mean_absolute_error: 0.6194\n",
            "Epoch 120/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5345 - mean_absolute_error: 0.5345 - val_loss: 0.6103 - val_mean_absolute_error: 0.6103\n",
            "Epoch 121/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5243 - mean_absolute_error: 0.5243 - val_loss: 0.6007 - val_mean_absolute_error: 0.6007\n",
            "Epoch 122/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5269 - mean_absolute_error: 0.5269 - val_loss: 0.6005 - val_mean_absolute_error: 0.6005\n",
            "Epoch 123/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5291 - mean_absolute_error: 0.5291 - val_loss: 0.6029 - val_mean_absolute_error: 0.6029\n",
            "Epoch 124/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5244 - mean_absolute_error: 0.5244 - val_loss: 0.5969 - val_mean_absolute_error: 0.5969\n",
            "Epoch 125/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_absolute_error: 0.5258 - val_loss: 0.6025 - val_mean_absolute_error: 0.6025\n",
            "Epoch 126/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5220 - mean_absolute_error: 0.5220 - val_loss: 0.6059 - val_mean_absolute_error: 0.6059\n",
            "Epoch 127/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5241 - mean_absolute_error: 0.5241 - val_loss: 0.5993 - val_mean_absolute_error: 0.5993\n",
            "Epoch 128/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5265 - mean_absolute_error: 0.5265 - val_loss: 0.5985 - val_mean_absolute_error: 0.5985\n",
            "Epoch 129/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5242 - mean_absolute_error: 0.5242 - val_loss: 0.5986 - val_mean_absolute_error: 0.5986\n",
            "Epoch 130/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_absolute_error: 0.5258 - val_loss: 0.6037 - val_mean_absolute_error: 0.6037\n",
            "Epoch 131/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_absolute_error: 0.5258 - val_loss: 0.6007 - val_mean_absolute_error: 0.6007\n",
            "Epoch 132/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5289 - mean_absolute_error: 0.5289 - val_loss: 0.6184 - val_mean_absolute_error: 0.6184\n",
            "Epoch 133/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5232 - mean_absolute_error: 0.5232 - val_loss: 0.5994 - val_mean_absolute_error: 0.5994\n",
            "Epoch 134/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5267 - mean_absolute_error: 0.5267 - val_loss: 0.5978 - val_mean_absolute_error: 0.5978\n",
            "Epoch 135/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5252 - mean_absolute_error: 0.5252 - val_loss: 0.6004 - val_mean_absolute_error: 0.6004\n",
            "Epoch 136/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5235 - mean_absolute_error: 0.5235 - val_loss: 0.5966 - val_mean_absolute_error: 0.5966\n",
            "Epoch 137/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5202 - mean_absolute_error: 0.5202 - val_loss: 0.5971 - val_mean_absolute_error: 0.5971\n",
            "Epoch 138/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5263 - mean_absolute_error: 0.5263 - val_loss: 0.5952 - val_mean_absolute_error: 0.5952\n",
            "Epoch 139/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5181 - mean_absolute_error: 0.5181 - val_loss: 0.5963 - val_mean_absolute_error: 0.5963\n",
            "Epoch 140/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5241 - mean_absolute_error: 0.5241 - val_loss: 0.5971 - val_mean_absolute_error: 0.5971\n",
            "Epoch 141/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5239 - mean_absolute_error: 0.5239 - val_loss: 0.6047 - val_mean_absolute_error: 0.6047\n",
            "Epoch 142/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5226 - mean_absolute_error: 0.5226 - val_loss: 0.5971 - val_mean_absolute_error: 0.5971\n",
            "Epoch 143/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5198 - mean_absolute_error: 0.5198 - val_loss: 0.5952 - val_mean_absolute_error: 0.5952\n",
            "Epoch 144/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5205 - mean_absolute_error: 0.5205 - val_loss: 0.5977 - val_mean_absolute_error: 0.5977\n",
            "Epoch 145/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5171 - mean_absolute_error: 0.5171 - val_loss: 0.5954 - val_mean_absolute_error: 0.5954\n",
            "Epoch 146/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5169 - mean_absolute_error: 0.5169 - val_loss: 0.5914 - val_mean_absolute_error: 0.5914\n",
            "Epoch 147/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5184 - mean_absolute_error: 0.5184 - val_loss: 0.5982 - val_mean_absolute_error: 0.5982\n",
            "Epoch 148/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5144 - mean_absolute_error: 0.5144 - val_loss: 0.6026 - val_mean_absolute_error: 0.6026\n",
            "Epoch 149/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5238 - mean_absolute_error: 0.5238 - val_loss: 0.5931 - val_mean_absolute_error: 0.5931\n",
            "Epoch 150/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5177 - mean_absolute_error: 0.5177 - val_loss: 0.5984 - val_mean_absolute_error: 0.5984\n",
            "Epoch 151/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5141 - mean_absolute_error: 0.5141 - val_loss: 0.5918 - val_mean_absolute_error: 0.5918\n",
            "Epoch 152/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5173 - mean_absolute_error: 0.5173 - val_loss: 0.5919 - val_mean_absolute_error: 0.5919\n",
            "Epoch 153/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5202 - mean_absolute_error: 0.5202 - val_loss: 0.5951 - val_mean_absolute_error: 0.5951\n",
            "Epoch 154/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5152 - mean_absolute_error: 0.5152 - val_loss: 0.5949 - val_mean_absolute_error: 0.5949\n",
            "Epoch 155/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5148 - mean_absolute_error: 0.5148 - val_loss: 0.5905 - val_mean_absolute_error: 0.5905\n",
            "Epoch 156/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5142 - mean_absolute_error: 0.5142 - val_loss: 0.5897 - val_mean_absolute_error: 0.5897\n",
            "Epoch 157/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5116 - mean_absolute_error: 0.5116 - val_loss: 0.5895 - val_mean_absolute_error: 0.5895\n",
            "Epoch 158/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5124 - mean_absolute_error: 0.5124 - val_loss: 0.5923 - val_mean_absolute_error: 0.5923\n",
            "Epoch 159/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5127 - mean_absolute_error: 0.5127 - val_loss: 0.5876 - val_mean_absolute_error: 0.5876\n",
            "Epoch 160/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5111 - mean_absolute_error: 0.5111 - val_loss: 0.5893 - val_mean_absolute_error: 0.5893\n",
            "Epoch 161/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5123 - mean_absolute_error: 0.5123 - val_loss: 0.5902 - val_mean_absolute_error: 0.5902\n",
            "Epoch 162/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5137 - mean_absolute_error: 0.5137 - val_loss: 0.5878 - val_mean_absolute_error: 0.5878\n",
            "Epoch 163/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5113 - mean_absolute_error: 0.5113 - val_loss: 0.5873 - val_mean_absolute_error: 0.5873\n",
            "Epoch 164/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5108 - mean_absolute_error: 0.5108 - val_loss: 0.5902 - val_mean_absolute_error: 0.5902\n",
            "Epoch 165/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5078 - mean_absolute_error: 0.5078 - val_loss: 0.5919 - val_mean_absolute_error: 0.5919\n",
            "Epoch 166/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5158 - mean_absolute_error: 0.5158 - val_loss: 0.5912 - val_mean_absolute_error: 0.5912\n",
            "Epoch 167/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5148 - mean_absolute_error: 0.5148 - val_loss: 0.5920 - val_mean_absolute_error: 0.5920\n",
            "Epoch 168/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5126 - mean_absolute_error: 0.5126 - val_loss: 0.5891 - val_mean_absolute_error: 0.5891\n",
            "Epoch 169/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5086 - mean_absolute_error: 0.5086 - val_loss: 0.5865 - val_mean_absolute_error: 0.5865\n",
            "Epoch 170/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5092 - mean_absolute_error: 0.5092 - val_loss: 0.5989 - val_mean_absolute_error: 0.5989\n",
            "Epoch 171/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5107 - mean_absolute_error: 0.5107 - val_loss: 0.5848 - val_mean_absolute_error: 0.5848\n",
            "Epoch 172/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5061 - mean_absolute_error: 0.5061 - val_loss: 0.6026 - val_mean_absolute_error: 0.6026\n",
            "Epoch 173/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5101 - mean_absolute_error: 0.5101 - val_loss: 0.5910 - val_mean_absolute_error: 0.5910\n",
            "Epoch 174/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5107 - mean_absolute_error: 0.5107 - val_loss: 0.5907 - val_mean_absolute_error: 0.5907\n",
            "Epoch 175/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5114 - mean_absolute_error: 0.5114 - val_loss: 0.5893 - val_mean_absolute_error: 0.5893\n",
            "Epoch 176/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5083 - mean_absolute_error: 0.5083 - val_loss: 0.5852 - val_mean_absolute_error: 0.5852\n",
            "Epoch 177/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5096 - mean_absolute_error: 0.5096 - val_loss: 0.5829 - val_mean_absolute_error: 0.5829\n",
            "Epoch 178/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - mean_absolute_error: 0.5041 - val_loss: 0.5850 - val_mean_absolute_error: 0.5850\n",
            "Epoch 179/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5106 - mean_absolute_error: 0.5106 - val_loss: 0.5885 - val_mean_absolute_error: 0.5885\n",
            "Epoch 180/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5063 - mean_absolute_error: 0.5063 - val_loss: 0.5879 - val_mean_absolute_error: 0.5879\n",
            "Epoch 181/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5087 - mean_absolute_error: 0.5087 - val_loss: 0.5818 - val_mean_absolute_error: 0.5818\n",
            "Epoch 182/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5114 - mean_absolute_error: 0.5114 - val_loss: 0.5813 - val_mean_absolute_error: 0.5813\n",
            "Epoch 183/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5052 - mean_absolute_error: 0.5052 - val_loss: 0.5816 - val_mean_absolute_error: 0.5816\n",
            "Epoch 184/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5020 - mean_absolute_error: 0.5020 - val_loss: 0.5824 - val_mean_absolute_error: 0.5824\n",
            "Epoch 185/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5041 - mean_absolute_error: 0.5041 - val_loss: 0.5807 - val_mean_absolute_error: 0.5807\n",
            "Epoch 186/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4994 - mean_absolute_error: 0.4994 - val_loss: 0.5794 - val_mean_absolute_error: 0.5794\n",
            "Epoch 187/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5053 - mean_absolute_error: 0.5053 - val_loss: 0.5881 - val_mean_absolute_error: 0.5881\n",
            "Epoch 188/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5024 - mean_absolute_error: 0.5024 - val_loss: 0.5853 - val_mean_absolute_error: 0.5853\n",
            "Epoch 189/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mean_absolute_error: 0.4989 - val_loss: 0.5785 - val_mean_absolute_error: 0.5785\n",
            "Epoch 190/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5002 - mean_absolute_error: 0.5002 - val_loss: 0.5798 - val_mean_absolute_error: 0.5798\n",
            "Epoch 191/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5055 - mean_absolute_error: 0.5055 - val_loss: 0.5810 - val_mean_absolute_error: 0.5810\n",
            "Epoch 192/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.5006 - mean_absolute_error: 0.5006 - val_loss: 0.5903 - val_mean_absolute_error: 0.5903\n",
            "Epoch 193/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4981 - mean_absolute_error: 0.4981 - val_loss: 0.5831 - val_mean_absolute_error: 0.5831\n",
            "Epoch 194/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4991 - mean_absolute_error: 0.4991 - val_loss: 0.5749 - val_mean_absolute_error: 0.5749\n",
            "Epoch 195/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4974 - mean_absolute_error: 0.4974 - val_loss: 0.5877 - val_mean_absolute_error: 0.5877\n",
            "Epoch 196/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4939 - mean_absolute_error: 0.4939 - val_loss: 0.5741 - val_mean_absolute_error: 0.5741\n",
            "Epoch 197/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4989 - mean_absolute_error: 0.4989 - val_loss: 0.5769 - val_mean_absolute_error: 0.5769\n",
            "Epoch 198/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4952 - mean_absolute_error: 0.4952 - val_loss: 0.5756 - val_mean_absolute_error: 0.5756\n",
            "Epoch 199/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4930 - mean_absolute_error: 0.4930 - val_loss: 0.5739 - val_mean_absolute_error: 0.5739\n",
            "Epoch 200/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4955 - mean_absolute_error: 0.4955 - val_loss: 0.5774 - val_mean_absolute_error: 0.5774\n",
            "Epoch 201/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4904 - mean_absolute_error: 0.4904 - val_loss: 0.5759 - val_mean_absolute_error: 0.5759\n",
            "Epoch 202/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4969 - mean_absolute_error: 0.4969 - val_loss: 0.5776 - val_mean_absolute_error: 0.5776\n",
            "Epoch 203/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4951 - mean_absolute_error: 0.4951 - val_loss: 0.5698 - val_mean_absolute_error: 0.5698\n",
            "Epoch 204/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4899 - mean_absolute_error: 0.4899 - val_loss: 0.5821 - val_mean_absolute_error: 0.5821\n",
            "Epoch 205/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4901 - mean_absolute_error: 0.4901 - val_loss: 0.5699 - val_mean_absolute_error: 0.5699\n",
            "Epoch 206/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4921 - mean_absolute_error: 0.4921 - val_loss: 0.5683 - val_mean_absolute_error: 0.5683\n",
            "Epoch 207/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4918 - mean_absolute_error: 0.4918 - val_loss: 0.5720 - val_mean_absolute_error: 0.5720\n",
            "Epoch 208/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4874 - mean_absolute_error: 0.4874 - val_loss: 0.5670 - val_mean_absolute_error: 0.5670\n",
            "Epoch 209/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4883 - mean_absolute_error: 0.4883 - val_loss: 0.5752 - val_mean_absolute_error: 0.5752\n",
            "Epoch 210/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4905 - mean_absolute_error: 0.4905 - val_loss: 0.5669 - val_mean_absolute_error: 0.5669\n",
            "Epoch 211/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4911 - mean_absolute_error: 0.4911 - val_loss: 0.5649 - val_mean_absolute_error: 0.5649\n",
            "Epoch 212/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4928 - mean_absolute_error: 0.4928 - val_loss: 0.5640 - val_mean_absolute_error: 0.5640\n",
            "Epoch 213/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4907 - mean_absolute_error: 0.4907 - val_loss: 0.5731 - val_mean_absolute_error: 0.5731\n",
            "Epoch 214/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4896 - mean_absolute_error: 0.4896 - val_loss: 0.5660 - val_mean_absolute_error: 0.5660\n",
            "Epoch 215/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4969 - mean_absolute_error: 0.4969 - val_loss: 0.5729 - val_mean_absolute_error: 0.5729\n",
            "Epoch 216/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4957 - mean_absolute_error: 0.4957 - val_loss: 0.5639 - val_mean_absolute_error: 0.5639\n",
            "Epoch 217/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4869 - mean_absolute_error: 0.4869 - val_loss: 0.5700 - val_mean_absolute_error: 0.5700\n",
            "Epoch 218/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4917 - mean_absolute_error: 0.4917 - val_loss: 0.5628 - val_mean_absolute_error: 0.5628\n",
            "Epoch 219/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4897 - mean_absolute_error: 0.4897 - val_loss: 0.5616 - val_mean_absolute_error: 0.5616\n",
            "Epoch 220/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4897 - mean_absolute_error: 0.4897 - val_loss: 0.5635 - val_mean_absolute_error: 0.5635\n",
            "Epoch 221/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4867 - mean_absolute_error: 0.4867 - val_loss: 0.5647 - val_mean_absolute_error: 0.5647\n",
            "Epoch 222/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4855 - mean_absolute_error: 0.4855 - val_loss: 0.5744 - val_mean_absolute_error: 0.5744\n",
            "Epoch 223/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4851 - mean_absolute_error: 0.4851 - val_loss: 0.5641 - val_mean_absolute_error: 0.5641\n",
            "Epoch 224/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4931 - mean_absolute_error: 0.4931 - val_loss: 0.5609 - val_mean_absolute_error: 0.5609\n",
            "Epoch 225/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4933 - mean_absolute_error: 0.4933 - val_loss: 0.5772 - val_mean_absolute_error: 0.5772\n",
            "Epoch 226/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4841 - mean_absolute_error: 0.4841 - val_loss: 0.5591 - val_mean_absolute_error: 0.5591\n",
            "Epoch 227/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4859 - mean_absolute_error: 0.4859 - val_loss: 0.5577 - val_mean_absolute_error: 0.5577\n",
            "Epoch 228/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4831 - mean_absolute_error: 0.4831 - val_loss: 0.5586 - val_mean_absolute_error: 0.5586\n",
            "Epoch 229/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4846 - mean_absolute_error: 0.4846 - val_loss: 0.5854 - val_mean_absolute_error: 0.5854\n",
            "Epoch 230/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4899 - mean_absolute_error: 0.4899 - val_loss: 0.5613 - val_mean_absolute_error: 0.5613\n",
            "Epoch 231/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4854 - mean_absolute_error: 0.4854 - val_loss: 0.5594 - val_mean_absolute_error: 0.5594\n",
            "Epoch 232/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4810 - mean_absolute_error: 0.4810 - val_loss: 0.5553 - val_mean_absolute_error: 0.5553\n",
            "Epoch 233/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4843 - mean_absolute_error: 0.4843 - val_loss: 0.5578 - val_mean_absolute_error: 0.5578\n",
            "Epoch 234/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4862 - mean_absolute_error: 0.4862 - val_loss: 0.5745 - val_mean_absolute_error: 0.5745\n",
            "Epoch 235/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4794 - mean_absolute_error: 0.4794 - val_loss: 0.5575 - val_mean_absolute_error: 0.5575\n",
            "Epoch 236/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4821 - mean_absolute_error: 0.4821 - val_loss: 0.5551 - val_mean_absolute_error: 0.5551\n",
            "Epoch 237/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4804 - mean_absolute_error: 0.4804 - val_loss: 0.5525 - val_mean_absolute_error: 0.5525\n",
            "Epoch 238/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4847 - mean_absolute_error: 0.4847 - val_loss: 0.5517 - val_mean_absolute_error: 0.5517\n",
            "Epoch 239/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4839 - mean_absolute_error: 0.4839 - val_loss: 0.5639 - val_mean_absolute_error: 0.5639\n",
            "Epoch 240/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4823 - mean_absolute_error: 0.4823 - val_loss: 0.5569 - val_mean_absolute_error: 0.5569\n",
            "Epoch 241/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4786 - mean_absolute_error: 0.4786 - val_loss: 0.5635 - val_mean_absolute_error: 0.5635\n",
            "Epoch 242/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4776 - mean_absolute_error: 0.4776 - val_loss: 0.5563 - val_mean_absolute_error: 0.5563\n",
            "Epoch 243/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4803 - mean_absolute_error: 0.4803 - val_loss: 0.5578 - val_mean_absolute_error: 0.5578\n",
            "Epoch 244/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4828 - mean_absolute_error: 0.4828 - val_loss: 0.5600 - val_mean_absolute_error: 0.5600\n",
            "Epoch 245/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4767 - mean_absolute_error: 0.4767 - val_loss: 0.5482 - val_mean_absolute_error: 0.5482\n",
            "Epoch 246/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4797 - mean_absolute_error: 0.4797 - val_loss: 0.5480 - val_mean_absolute_error: 0.5480\n",
            "Epoch 247/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4781 - mean_absolute_error: 0.4781 - val_loss: 0.5593 - val_mean_absolute_error: 0.5593\n",
            "Epoch 248/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4820 - mean_absolute_error: 0.4820 - val_loss: 0.5497 - val_mean_absolute_error: 0.5497\n",
            "Epoch 249/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4864 - mean_absolute_error: 0.4864 - val_loss: 0.5511 - val_mean_absolute_error: 0.5511\n",
            "Epoch 250/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4794 - mean_absolute_error: 0.4794 - val_loss: 0.5490 - val_mean_absolute_error: 0.5490\n",
            "Epoch 251/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4814 - mean_absolute_error: 0.4814 - val_loss: 0.5472 - val_mean_absolute_error: 0.5472\n",
            "Epoch 252/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4791 - mean_absolute_error: 0.4791 - val_loss: 0.5679 - val_mean_absolute_error: 0.5679\n",
            "Epoch 253/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4764 - mean_absolute_error: 0.4764 - val_loss: 0.5509 - val_mean_absolute_error: 0.5509\n",
            "Epoch 254/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4745 - mean_absolute_error: 0.4745 - val_loss: 0.5475 - val_mean_absolute_error: 0.5475\n",
            "Epoch 255/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4790 - mean_absolute_error: 0.4790 - val_loss: 0.5572 - val_mean_absolute_error: 0.5572\n",
            "Epoch 256/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4727 - mean_absolute_error: 0.4727 - val_loss: 0.5546 - val_mean_absolute_error: 0.5546\n",
            "Epoch 257/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4788 - mean_absolute_error: 0.4788 - val_loss: 0.5476 - val_mean_absolute_error: 0.5476\n",
            "Epoch 258/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4739 - mean_absolute_error: 0.4739 - val_loss: 0.5454 - val_mean_absolute_error: 0.5454\n",
            "Epoch 259/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4766 - mean_absolute_error: 0.4766 - val_loss: 0.5460 - val_mean_absolute_error: 0.5460\n",
            "Epoch 260/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4736 - mean_absolute_error: 0.4736 - val_loss: 0.5517 - val_mean_absolute_error: 0.5517\n",
            "Epoch 261/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4779 - mean_absolute_error: 0.4779 - val_loss: 0.5443 - val_mean_absolute_error: 0.5443\n",
            "Epoch 262/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4829 - mean_absolute_error: 0.4829 - val_loss: 0.5578 - val_mean_absolute_error: 0.5578\n",
            "Epoch 263/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4795 - mean_absolute_error: 0.4795 - val_loss: 0.5456 - val_mean_absolute_error: 0.5456\n",
            "Epoch 264/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4781 - mean_absolute_error: 0.4781 - val_loss: 0.5603 - val_mean_absolute_error: 0.5603\n",
            "Epoch 265/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4784 - mean_absolute_error: 0.4784 - val_loss: 0.5465 - val_mean_absolute_error: 0.5465\n",
            "Epoch 266/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4741 - mean_absolute_error: 0.4741 - val_loss: 0.5473 - val_mean_absolute_error: 0.5473\n",
            "Epoch 267/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4737 - mean_absolute_error: 0.4737 - val_loss: 0.5467 - val_mean_absolute_error: 0.5467\n",
            "Epoch 268/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4717 - mean_absolute_error: 0.4717 - val_loss: 0.5490 - val_mean_absolute_error: 0.5490\n",
            "Epoch 269/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4692 - mean_absolute_error: 0.4692 - val_loss: 0.5439 - val_mean_absolute_error: 0.5439\n",
            "Epoch 270/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4716 - mean_absolute_error: 0.4716 - val_loss: 0.5495 - val_mean_absolute_error: 0.5495\n",
            "Epoch 271/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4707 - mean_absolute_error: 0.4707 - val_loss: 0.5445 - val_mean_absolute_error: 0.5445\n",
            "Epoch 272/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4695 - mean_absolute_error: 0.4695 - val_loss: 0.5775 - val_mean_absolute_error: 0.5775\n",
            "Epoch 273/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4794 - mean_absolute_error: 0.4794 - val_loss: 0.5406 - val_mean_absolute_error: 0.5406\n",
            "Epoch 274/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4837 - mean_absolute_error: 0.4837 - val_loss: 0.5480 - val_mean_absolute_error: 0.5480\n",
            "Epoch 275/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4688 - mean_absolute_error: 0.4688 - val_loss: 0.5395 - val_mean_absolute_error: 0.5395\n",
            "Epoch 276/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4679 - mean_absolute_error: 0.4679 - val_loss: 0.5491 - val_mean_absolute_error: 0.5491\n",
            "Epoch 277/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4707 - mean_absolute_error: 0.4707 - val_loss: 0.5488 - val_mean_absolute_error: 0.5488\n",
            "Epoch 278/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4693 - mean_absolute_error: 0.4693 - val_loss: 0.5494 - val_mean_absolute_error: 0.5494\n",
            "Epoch 279/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4722 - mean_absolute_error: 0.4722 - val_loss: 0.5488 - val_mean_absolute_error: 0.5488\n",
            "Epoch 280/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4701 - mean_absolute_error: 0.4701 - val_loss: 0.5447 - val_mean_absolute_error: 0.5447\n",
            "Epoch 281/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4704 - mean_absolute_error: 0.4704 - val_loss: 0.5426 - val_mean_absolute_error: 0.5426\n",
            "Epoch 282/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4694 - mean_absolute_error: 0.4694 - val_loss: 0.5482 - val_mean_absolute_error: 0.5482\n",
            "Epoch 283/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4707 - mean_absolute_error: 0.4707 - val_loss: 0.5407 - val_mean_absolute_error: 0.5407\n",
            "Epoch 284/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4663 - mean_absolute_error: 0.4663 - val_loss: 0.5479 - val_mean_absolute_error: 0.5479\n",
            "Epoch 285/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4686 - mean_absolute_error: 0.4686 - val_loss: 0.5508 - val_mean_absolute_error: 0.5508\n",
            "Epoch 286/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4733 - mean_absolute_error: 0.4733 - val_loss: 0.5497 - val_mean_absolute_error: 0.5497\n",
            "Epoch 287/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4689 - mean_absolute_error: 0.4689 - val_loss: 0.5446 - val_mean_absolute_error: 0.5446\n",
            "Epoch 288/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4796 - mean_absolute_error: 0.4796 - val_loss: 0.5377 - val_mean_absolute_error: 0.5377\n",
            "Epoch 289/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4703 - mean_absolute_error: 0.4703 - val_loss: 0.5589 - val_mean_absolute_error: 0.5589\n",
            "Epoch 290/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4727 - mean_absolute_error: 0.4727 - val_loss: 0.5345 - val_mean_absolute_error: 0.5345\n",
            "Epoch 291/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4748 - mean_absolute_error: 0.4748 - val_loss: 0.5630 - val_mean_absolute_error: 0.5630\n",
            "Epoch 292/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4696 - mean_absolute_error: 0.4696 - val_loss: 0.5415 - val_mean_absolute_error: 0.5415\n",
            "Epoch 293/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4663 - mean_absolute_error: 0.4663 - val_loss: 0.5386 - val_mean_absolute_error: 0.5386\n",
            "Epoch 294/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4640 - mean_absolute_error: 0.4640 - val_loss: 0.5326 - val_mean_absolute_error: 0.5326\n",
            "Epoch 295/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4658 - mean_absolute_error: 0.4658 - val_loss: 0.5341 - val_mean_absolute_error: 0.5341\n",
            "Epoch 296/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4635 - mean_absolute_error: 0.4635 - val_loss: 0.5303 - val_mean_absolute_error: 0.5303\n",
            "Epoch 297/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4638 - mean_absolute_error: 0.4638 - val_loss: 0.5309 - val_mean_absolute_error: 0.5309\n",
            "Epoch 298/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4656 - mean_absolute_error: 0.4656 - val_loss: 0.5333 - val_mean_absolute_error: 0.5333\n",
            "Epoch 299/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4673 - mean_absolute_error: 0.4673 - val_loss: 0.5427 - val_mean_absolute_error: 0.5427\n",
            "Epoch 300/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4627 - mean_absolute_error: 0.4627 - val_loss: 0.5304 - val_mean_absolute_error: 0.5304\n",
            "Epoch 301/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4626 - mean_absolute_error: 0.4626 - val_loss: 0.5340 - val_mean_absolute_error: 0.5340\n",
            "Epoch 302/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4689 - mean_absolute_error: 0.4689 - val_loss: 0.5405 - val_mean_absolute_error: 0.5405\n",
            "Epoch 303/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4654 - mean_absolute_error: 0.4654 - val_loss: 0.5385 - val_mean_absolute_error: 0.5385\n",
            "Epoch 304/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4714 - mean_absolute_error: 0.4714 - val_loss: 0.5583 - val_mean_absolute_error: 0.5583\n",
            "Epoch 305/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4698 - mean_absolute_error: 0.4698 - val_loss: 0.5413 - val_mean_absolute_error: 0.5413\n",
            "Epoch 306/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4644 - mean_absolute_error: 0.4644 - val_loss: 0.5336 - val_mean_absolute_error: 0.5336\n",
            "Epoch 307/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4633 - mean_absolute_error: 0.4633 - val_loss: 0.5349 - val_mean_absolute_error: 0.5349\n",
            "Epoch 308/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4617 - mean_absolute_error: 0.4617 - val_loss: 0.5313 - val_mean_absolute_error: 0.5313\n",
            "Epoch 309/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4619 - mean_absolute_error: 0.4619 - val_loss: 0.5293 - val_mean_absolute_error: 0.5293\n",
            "Epoch 310/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4736 - mean_absolute_error: 0.4736 - val_loss: 0.5545 - val_mean_absolute_error: 0.5545\n",
            "Epoch 311/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4861 - mean_absolute_error: 0.4861 - val_loss: 0.5310 - val_mean_absolute_error: 0.5310\n",
            "Epoch 312/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4645 - mean_absolute_error: 0.4645 - val_loss: 0.5293 - val_mean_absolute_error: 0.5293\n",
            "Epoch 313/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4688 - mean_absolute_error: 0.4688 - val_loss: 0.5316 - val_mean_absolute_error: 0.5316\n",
            "Epoch 314/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4635 - mean_absolute_error: 0.4635 - val_loss: 0.5369 - val_mean_absolute_error: 0.5369\n",
            "Epoch 315/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4620 - mean_absolute_error: 0.4620 - val_loss: 0.5353 - val_mean_absolute_error: 0.5353\n",
            "Epoch 316/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4632 - mean_absolute_error: 0.4632 - val_loss: 0.5279 - val_mean_absolute_error: 0.5279\n",
            "Epoch 317/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4634 - mean_absolute_error: 0.4634 - val_loss: 0.5422 - val_mean_absolute_error: 0.5422\n",
            "Epoch 318/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4612 - mean_absolute_error: 0.4612 - val_loss: 0.5312 - val_mean_absolute_error: 0.5312\n",
            "Epoch 319/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4608 - mean_absolute_error: 0.4608 - val_loss: 0.5303 - val_mean_absolute_error: 0.5303\n",
            "Epoch 320/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4648 - mean_absolute_error: 0.4648 - val_loss: 0.5266 - val_mean_absolute_error: 0.5266\n",
            "Epoch 321/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4616 - mean_absolute_error: 0.4616 - val_loss: 0.5331 - val_mean_absolute_error: 0.5331\n",
            "Epoch 322/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4597 - mean_absolute_error: 0.4597 - val_loss: 0.5387 - val_mean_absolute_error: 0.5387\n",
            "Epoch 323/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4696 - mean_absolute_error: 0.4696 - val_loss: 0.5329 - val_mean_absolute_error: 0.5329\n",
            "Epoch 324/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4707 - mean_absolute_error: 0.4707 - val_loss: 0.5450 - val_mean_absolute_error: 0.5450\n",
            "Epoch 325/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4607 - mean_absolute_error: 0.4607 - val_loss: 0.5274 - val_mean_absolute_error: 0.5274\n",
            "Epoch 326/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4591 - mean_absolute_error: 0.4591 - val_loss: 0.5284 - val_mean_absolute_error: 0.5284\n",
            "Epoch 327/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4584 - mean_absolute_error: 0.4584 - val_loss: 0.5325 - val_mean_absolute_error: 0.5325\n",
            "Epoch 328/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4575 - mean_absolute_error: 0.4575 - val_loss: 0.5274 - val_mean_absolute_error: 0.5274\n",
            "Epoch 329/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4570 - mean_absolute_error: 0.4570 - val_loss: 0.5274 - val_mean_absolute_error: 0.5274\n",
            "Epoch 330/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4590 - mean_absolute_error: 0.4590 - val_loss: 0.5250 - val_mean_absolute_error: 0.5250\n",
            "Epoch 331/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4629 - mean_absolute_error: 0.4629 - val_loss: 0.5236 - val_mean_absolute_error: 0.5236\n",
            "Epoch 332/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4577 - mean_absolute_error: 0.4577 - val_loss: 0.5256 - val_mean_absolute_error: 0.5256\n",
            "Epoch 333/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4586 - mean_absolute_error: 0.4586 - val_loss: 0.5250 - val_mean_absolute_error: 0.5250\n",
            "Epoch 334/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4594 - mean_absolute_error: 0.4594 - val_loss: 0.5266 - val_mean_absolute_error: 0.5266\n",
            "Epoch 335/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4570 - mean_absolute_error: 0.4570 - val_loss: 0.5236 - val_mean_absolute_error: 0.5236\n",
            "Epoch 336/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4574 - mean_absolute_error: 0.4574 - val_loss: 0.5284 - val_mean_absolute_error: 0.5284\n",
            "Epoch 337/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4580 - mean_absolute_error: 0.4580 - val_loss: 0.5217 - val_mean_absolute_error: 0.5217\n",
            "Epoch 338/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4582 - mean_absolute_error: 0.4582 - val_loss: 0.5244 - val_mean_absolute_error: 0.5244\n",
            "Epoch 339/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4568 - mean_absolute_error: 0.4568 - val_loss: 0.5240 - val_mean_absolute_error: 0.5240\n",
            "Epoch 340/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4566 - mean_absolute_error: 0.4566 - val_loss: 0.5196 - val_mean_absolute_error: 0.5196\n",
            "Epoch 341/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4575 - mean_absolute_error: 0.4575 - val_loss: 0.5203 - val_mean_absolute_error: 0.5203\n",
            "Epoch 342/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4551 - mean_absolute_error: 0.4551 - val_loss: 0.5227 - val_mean_absolute_error: 0.5227\n",
            "Epoch 343/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4554 - mean_absolute_error: 0.4554 - val_loss: 0.5199 - val_mean_absolute_error: 0.5199\n",
            "Epoch 344/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4603 - mean_absolute_error: 0.4603 - val_loss: 0.5210 - val_mean_absolute_error: 0.5210\n",
            "Epoch 345/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4576 - mean_absolute_error: 0.4576 - val_loss: 0.5204 - val_mean_absolute_error: 0.5204\n",
            "Epoch 346/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4596 - mean_absolute_error: 0.4596 - val_loss: 0.5266 - val_mean_absolute_error: 0.5266\n",
            "Epoch 347/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4524 - mean_absolute_error: 0.4524 - val_loss: 0.5164 - val_mean_absolute_error: 0.5164\n",
            "Epoch 348/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4563 - mean_absolute_error: 0.4563 - val_loss: 0.5409 - val_mean_absolute_error: 0.5409\n",
            "Epoch 349/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4556 - mean_absolute_error: 0.4556 - val_loss: 0.5160 - val_mean_absolute_error: 0.5160\n",
            "Epoch 350/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4622 - mean_absolute_error: 0.4622 - val_loss: 0.5211 - val_mean_absolute_error: 0.5211\n",
            "Epoch 351/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4557 - mean_absolute_error: 0.4557 - val_loss: 0.5206 - val_mean_absolute_error: 0.5206\n",
            "Epoch 352/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4528 - mean_absolute_error: 0.4528 - val_loss: 0.5348 - val_mean_absolute_error: 0.5348\n",
            "Epoch 353/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4660 - mean_absolute_error: 0.4660 - val_loss: 0.5130 - val_mean_absolute_error: 0.5130\n",
            "Epoch 354/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4569 - mean_absolute_error: 0.4569 - val_loss: 0.5150 - val_mean_absolute_error: 0.5150\n",
            "Epoch 355/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4584 - mean_absolute_error: 0.4584 - val_loss: 0.5227 - val_mean_absolute_error: 0.5227\n",
            "Epoch 356/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4547 - mean_absolute_error: 0.4547 - val_loss: 0.5161 - val_mean_absolute_error: 0.5161\n",
            "Epoch 357/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4491 - mean_absolute_error: 0.4491 - val_loss: 0.5220 - val_mean_absolute_error: 0.5220\n",
            "Epoch 358/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4537 - mean_absolute_error: 0.4537 - val_loss: 0.5117 - val_mean_absolute_error: 0.5117\n",
            "Epoch 359/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4498 - mean_absolute_error: 0.4498 - val_loss: 0.5107 - val_mean_absolute_error: 0.5107\n",
            "Epoch 360/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4559 - mean_absolute_error: 0.4559 - val_loss: 0.5152 - val_mean_absolute_error: 0.5152\n",
            "Epoch 361/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4519 - mean_absolute_error: 0.4519 - val_loss: 0.5136 - val_mean_absolute_error: 0.5136\n",
            "Epoch 362/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4533 - mean_absolute_error: 0.4533 - val_loss: 0.5103 - val_mean_absolute_error: 0.5103\n",
            "Epoch 363/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4601 - mean_absolute_error: 0.4601 - val_loss: 0.5296 - val_mean_absolute_error: 0.5296\n",
            "Epoch 364/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4608 - mean_absolute_error: 0.4608 - val_loss: 0.5116 - val_mean_absolute_error: 0.5116\n",
            "Epoch 365/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4549 - mean_absolute_error: 0.4549 - val_loss: 0.5206 - val_mean_absolute_error: 0.5206\n",
            "Epoch 366/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4489 - mean_absolute_error: 0.4489 - val_loss: 0.5183 - val_mean_absolute_error: 0.5183\n",
            "Epoch 367/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4535 - mean_absolute_error: 0.4535 - val_loss: 0.5074 - val_mean_absolute_error: 0.5074\n",
            "Epoch 368/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4523 - mean_absolute_error: 0.4523 - val_loss: 0.5058 - val_mean_absolute_error: 0.5058\n",
            "Epoch 369/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4664 - mean_absolute_error: 0.4664 - val_loss: 0.5315 - val_mean_absolute_error: 0.5315\n",
            "Epoch 370/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4487 - mean_absolute_error: 0.4487 - val_loss: 0.5105 - val_mean_absolute_error: 0.5105\n",
            "Epoch 371/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4515 - mean_absolute_error: 0.4515 - val_loss: 0.5059 - val_mean_absolute_error: 0.5059\n",
            "Epoch 372/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4467 - mean_absolute_error: 0.4467 - val_loss: 0.5271 - val_mean_absolute_error: 0.5271\n",
            "Epoch 373/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4500 - mean_absolute_error: 0.4500 - val_loss: 0.5060 - val_mean_absolute_error: 0.5060\n",
            "Epoch 374/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4468 - mean_absolute_error: 0.4468 - val_loss: 0.5249 - val_mean_absolute_error: 0.5249\n",
            "Epoch 375/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4568 - mean_absolute_error: 0.4568 - val_loss: 0.5098 - val_mean_absolute_error: 0.5098\n",
            "Epoch 376/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4484 - mean_absolute_error: 0.4484 - val_loss: 0.5026 - val_mean_absolute_error: 0.5026\n",
            "Epoch 377/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4521 - mean_absolute_error: 0.4521 - val_loss: 0.5024 - val_mean_absolute_error: 0.5024\n",
            "Epoch 378/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4578 - mean_absolute_error: 0.4578 - val_loss: 0.4998 - val_mean_absolute_error: 0.4998\n",
            "Epoch 379/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4462 - mean_absolute_error: 0.4462 - val_loss: 0.5132 - val_mean_absolute_error: 0.5132\n",
            "Epoch 380/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4417 - mean_absolute_error: 0.4417 - val_loss: 0.5002 - val_mean_absolute_error: 0.5002\n",
            "Epoch 381/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4437 - mean_absolute_error: 0.4437 - val_loss: 0.4991 - val_mean_absolute_error: 0.4991\n",
            "Epoch 382/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4495 - mean_absolute_error: 0.4495 - val_loss: 0.5318 - val_mean_absolute_error: 0.5318\n",
            "Epoch 383/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4514 - mean_absolute_error: 0.4514 - val_loss: 0.5160 - val_mean_absolute_error: 0.5160\n",
            "Epoch 384/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4439 - mean_absolute_error: 0.4439 - val_loss: 0.4968 - val_mean_absolute_error: 0.4968\n",
            "Epoch 385/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4470 - mean_absolute_error: 0.4470 - val_loss: 0.5160 - val_mean_absolute_error: 0.5160\n",
            "Epoch 386/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4442 - mean_absolute_error: 0.4442 - val_loss: 0.4966 - val_mean_absolute_error: 0.4966\n",
            "Epoch 387/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4418 - mean_absolute_error: 0.4418 - val_loss: 0.4970 - val_mean_absolute_error: 0.4970\n",
            "Epoch 388/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4460 - mean_absolute_error: 0.4460 - val_loss: 0.5037 - val_mean_absolute_error: 0.5037\n",
            "Epoch 389/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4576 - mean_absolute_error: 0.4576 - val_loss: 0.4948 - val_mean_absolute_error: 0.4948\n",
            "Epoch 390/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4414 - mean_absolute_error: 0.4414 - val_loss: 0.4994 - val_mean_absolute_error: 0.4994\n",
            "Epoch 391/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4414 - mean_absolute_error: 0.4414 - val_loss: 0.5042 - val_mean_absolute_error: 0.5042\n",
            "Epoch 392/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4424 - mean_absolute_error: 0.4424 - val_loss: 0.4970 - val_mean_absolute_error: 0.4970\n",
            "Epoch 393/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4424 - mean_absolute_error: 0.4424 - val_loss: 0.4969 - val_mean_absolute_error: 0.4969\n",
            "Epoch 394/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4410 - mean_absolute_error: 0.4410 - val_loss: 0.5036 - val_mean_absolute_error: 0.5036\n",
            "Epoch 395/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4376 - mean_absolute_error: 0.4376 - val_loss: 0.5075 - val_mean_absolute_error: 0.5075\n",
            "Epoch 396/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4409 - mean_absolute_error: 0.4409 - val_loss: 0.4909 - val_mean_absolute_error: 0.4909\n",
            "Epoch 397/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4405 - mean_absolute_error: 0.4405 - val_loss: 0.5009 - val_mean_absolute_error: 0.5009\n",
            "Epoch 398/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4393 - mean_absolute_error: 0.4393 - val_loss: 0.4920 - val_mean_absolute_error: 0.4920\n",
            "Epoch 399/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4380 - mean_absolute_error: 0.4380 - val_loss: 0.5044 - val_mean_absolute_error: 0.5044\n",
            "Epoch 400/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4473 - mean_absolute_error: 0.4473 - val_loss: 0.4971 - val_mean_absolute_error: 0.4971\n",
            "Epoch 401/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4410 - mean_absolute_error: 0.4410 - val_loss: 0.4916 - val_mean_absolute_error: 0.4916\n",
            "Epoch 402/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4415 - mean_absolute_error: 0.4415 - val_loss: 0.4933 - val_mean_absolute_error: 0.4933\n",
            "Epoch 403/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4399 - mean_absolute_error: 0.4399 - val_loss: 0.4924 - val_mean_absolute_error: 0.4924\n",
            "Epoch 404/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4392 - mean_absolute_error: 0.4392 - val_loss: 0.4967 - val_mean_absolute_error: 0.4967\n",
            "Epoch 405/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4380 - mean_absolute_error: 0.4380 - val_loss: 0.4898 - val_mean_absolute_error: 0.4898\n",
            "Epoch 406/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4437 - mean_absolute_error: 0.4437 - val_loss: 0.4882 - val_mean_absolute_error: 0.4882\n",
            "Epoch 407/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4365 - mean_absolute_error: 0.4365 - val_loss: 0.4893 - val_mean_absolute_error: 0.4893\n",
            "Epoch 408/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4360 - mean_absolute_error: 0.4360 - val_loss: 0.4923 - val_mean_absolute_error: 0.4923\n",
            "Epoch 409/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4378 - mean_absolute_error: 0.4378 - val_loss: 0.4916 - val_mean_absolute_error: 0.4916\n",
            "Epoch 410/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4370 - mean_absolute_error: 0.4370 - val_loss: 0.4898 - val_mean_absolute_error: 0.4898\n",
            "Epoch 411/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4373 - mean_absolute_error: 0.4373 - val_loss: 0.4854 - val_mean_absolute_error: 0.4854\n",
            "Epoch 412/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4418 - mean_absolute_error: 0.4418 - val_loss: 0.5001 - val_mean_absolute_error: 0.5001\n",
            "Epoch 413/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4355 - mean_absolute_error: 0.4355 - val_loss: 0.4868 - val_mean_absolute_error: 0.4868\n",
            "Epoch 414/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4335 - mean_absolute_error: 0.4335 - val_loss: 0.4896 - val_mean_absolute_error: 0.4896\n",
            "Epoch 415/500\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.4356 - mean_absolute_error: 0.4356 - val_loss: 0.4846 - val_mean_absolute_error: 0.4846\n",
            "Epoch 416/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4323 - mean_absolute_error: 0.4323 - val_loss: 0.4944 - val_mean_absolute_error: 0.4944\n",
            "Epoch 417/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4375 - mean_absolute_error: 0.4375 - val_loss: 0.4873 - val_mean_absolute_error: 0.4873\n",
            "Epoch 418/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4372 - mean_absolute_error: 0.4372 - val_loss: 0.4987 - val_mean_absolute_error: 0.4987\n",
            "Epoch 419/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4355 - mean_absolute_error: 0.4355 - val_loss: 0.4819 - val_mean_absolute_error: 0.4819\n",
            "Epoch 420/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4334 - mean_absolute_error: 0.4334 - val_loss: 0.4996 - val_mean_absolute_error: 0.4996\n",
            "Epoch 421/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4408 - mean_absolute_error: 0.4408 - val_loss: 0.4859 - val_mean_absolute_error: 0.4859\n",
            "Epoch 422/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4369 - mean_absolute_error: 0.4369 - val_loss: 0.4806 - val_mean_absolute_error: 0.4806\n",
            "Epoch 423/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4346 - mean_absolute_error: 0.4346 - val_loss: 0.4838 - val_mean_absolute_error: 0.4838\n",
            "Epoch 424/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4337 - mean_absolute_error: 0.4337 - val_loss: 0.4821 - val_mean_absolute_error: 0.4821\n",
            "Epoch 425/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4334 - mean_absolute_error: 0.4334 - val_loss: 0.4865 - val_mean_absolute_error: 0.4865\n",
            "Epoch 426/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4319 - mean_absolute_error: 0.4319 - val_loss: 0.4796 - val_mean_absolute_error: 0.4796\n",
            "Epoch 427/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4323 - mean_absolute_error: 0.4323 - val_loss: 0.4948 - val_mean_absolute_error: 0.4948\n",
            "Epoch 428/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4356 - mean_absolute_error: 0.4356 - val_loss: 0.4841 - val_mean_absolute_error: 0.4841\n",
            "Epoch 429/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4432 - mean_absolute_error: 0.4432 - val_loss: 0.4861 - val_mean_absolute_error: 0.4861\n",
            "Epoch 430/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4347 - mean_absolute_error: 0.4347 - val_loss: 0.4948 - val_mean_absolute_error: 0.4948\n",
            "Epoch 431/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4317 - mean_absolute_error: 0.4317 - val_loss: 0.4800 - val_mean_absolute_error: 0.4800\n",
            "Epoch 432/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4327 - mean_absolute_error: 0.4327 - val_loss: 0.4820 - val_mean_absolute_error: 0.4820\n",
            "Epoch 433/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4324 - mean_absolute_error: 0.4324 - val_loss: 0.5021 - val_mean_absolute_error: 0.5021\n",
            "Epoch 434/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4515 - mean_absolute_error: 0.4515 - val_loss: 0.4790 - val_mean_absolute_error: 0.4790\n",
            "Epoch 435/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4384 - mean_absolute_error: 0.4384 - val_loss: 0.4846 - val_mean_absolute_error: 0.4846\n",
            "Epoch 436/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4299 - mean_absolute_error: 0.4299 - val_loss: 0.4791 - val_mean_absolute_error: 0.4791\n",
            "Epoch 437/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4264 - mean_absolute_error: 0.4264 - val_loss: 0.4784 - val_mean_absolute_error: 0.4784\n",
            "Epoch 438/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4266 - mean_absolute_error: 0.4266 - val_loss: 0.5128 - val_mean_absolute_error: 0.5128\n",
            "Epoch 439/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4359 - mean_absolute_error: 0.4359 - val_loss: 0.4796 - val_mean_absolute_error: 0.4796\n",
            "Epoch 440/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4315 - mean_absolute_error: 0.4315 - val_loss: 0.4767 - val_mean_absolute_error: 0.4767\n",
            "Epoch 441/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4307 - mean_absolute_error: 0.4307 - val_loss: 0.4778 - val_mean_absolute_error: 0.4778\n",
            "Epoch 442/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4332 - mean_absolute_error: 0.4332 - val_loss: 0.4773 - val_mean_absolute_error: 0.4773\n",
            "Epoch 443/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4299 - mean_absolute_error: 0.4299 - val_loss: 0.4744 - val_mean_absolute_error: 0.4744\n",
            "Epoch 444/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4323 - mean_absolute_error: 0.4323 - val_loss: 0.4839 - val_mean_absolute_error: 0.4839\n",
            "Epoch 445/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4300 - mean_absolute_error: 0.4300 - val_loss: 0.4881 - val_mean_absolute_error: 0.4881\n",
            "Epoch 446/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4297 - mean_absolute_error: 0.4297 - val_loss: 0.4744 - val_mean_absolute_error: 0.4744\n",
            "Epoch 447/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4313 - mean_absolute_error: 0.4313 - val_loss: 0.5005 - val_mean_absolute_error: 0.5005\n",
            "Epoch 448/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4296 - mean_absolute_error: 0.4296 - val_loss: 0.4906 - val_mean_absolute_error: 0.4906\n",
            "Epoch 449/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4377 - mean_absolute_error: 0.4377 - val_loss: 0.4731 - val_mean_absolute_error: 0.4731\n",
            "Epoch 450/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4317 - mean_absolute_error: 0.4317 - val_loss: 0.4823 - val_mean_absolute_error: 0.4823\n",
            "Epoch 451/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4293 - mean_absolute_error: 0.4293 - val_loss: 0.4951 - val_mean_absolute_error: 0.4951\n",
            "Epoch 452/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4336 - mean_absolute_error: 0.4336 - val_loss: 0.4790 - val_mean_absolute_error: 0.4790\n",
            "Epoch 453/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4368 - mean_absolute_error: 0.4368 - val_loss: 0.4729 - val_mean_absolute_error: 0.4729\n",
            "Epoch 454/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4303 - mean_absolute_error: 0.4303 - val_loss: 0.4851 - val_mean_absolute_error: 0.4851\n",
            "Epoch 455/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4275 - mean_absolute_error: 0.4275 - val_loss: 0.4715 - val_mean_absolute_error: 0.4715\n",
            "Epoch 456/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4254 - mean_absolute_error: 0.4254 - val_loss: 0.4840 - val_mean_absolute_error: 0.4840\n",
            "Epoch 457/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4291 - mean_absolute_error: 0.4291 - val_loss: 0.4845 - val_mean_absolute_error: 0.4845\n",
            "Epoch 458/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4266 - mean_absolute_error: 0.4266 - val_loss: 0.4837 - val_mean_absolute_error: 0.4837\n",
            "Epoch 459/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4274 - mean_absolute_error: 0.4274 - val_loss: 0.4711 - val_mean_absolute_error: 0.4711\n",
            "Epoch 460/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4288 - mean_absolute_error: 0.4288 - val_loss: 0.4730 - val_mean_absolute_error: 0.4730\n",
            "Epoch 461/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4310 - mean_absolute_error: 0.4310 - val_loss: 0.4749 - val_mean_absolute_error: 0.4749\n",
            "Epoch 462/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4274 - mean_absolute_error: 0.4274 - val_loss: 0.4729 - val_mean_absolute_error: 0.4729\n",
            "Epoch 463/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4321 - mean_absolute_error: 0.4321 - val_loss: 0.4753 - val_mean_absolute_error: 0.4753\n",
            "Epoch 464/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4235 - mean_absolute_error: 0.4235 - val_loss: 0.4697 - val_mean_absolute_error: 0.4697\n",
            "Epoch 465/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4313 - mean_absolute_error: 0.4313 - val_loss: 0.4994 - val_mean_absolute_error: 0.4994\n",
            "Epoch 466/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4257 - mean_absolute_error: 0.4257 - val_loss: 0.4721 - val_mean_absolute_error: 0.4721\n",
            "Epoch 467/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4320 - mean_absolute_error: 0.4320 - val_loss: 0.4775 - val_mean_absolute_error: 0.4775\n",
            "Epoch 468/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4297 - mean_absolute_error: 0.4297 - val_loss: 0.4719 - val_mean_absolute_error: 0.4719\n",
            "Epoch 469/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4242 - mean_absolute_error: 0.4242 - val_loss: 0.4816 - val_mean_absolute_error: 0.4816\n",
            "Epoch 470/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4293 - mean_absolute_error: 0.4293 - val_loss: 0.4669 - val_mean_absolute_error: 0.4669\n",
            "Epoch 471/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4238 - mean_absolute_error: 0.4238 - val_loss: 0.4649 - val_mean_absolute_error: 0.4649\n",
            "Epoch 472/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4231 - mean_absolute_error: 0.4231 - val_loss: 0.4799 - val_mean_absolute_error: 0.4799\n",
            "Epoch 473/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4315 - mean_absolute_error: 0.4315 - val_loss: 0.4661 - val_mean_absolute_error: 0.4661\n",
            "Epoch 474/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4283 - mean_absolute_error: 0.4283 - val_loss: 0.4654 - val_mean_absolute_error: 0.4654\n",
            "Epoch 475/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4326 - mean_absolute_error: 0.4326 - val_loss: 0.4972 - val_mean_absolute_error: 0.4972\n",
            "Epoch 476/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4296 - mean_absolute_error: 0.4296 - val_loss: 0.4782 - val_mean_absolute_error: 0.4782\n",
            "Epoch 477/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4332 - mean_absolute_error: 0.4332 - val_loss: 0.4677 - val_mean_absolute_error: 0.4677\n",
            "Epoch 478/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4320 - mean_absolute_error: 0.4320 - val_loss: 0.4731 - val_mean_absolute_error: 0.4731\n",
            "Epoch 479/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4212 - mean_absolute_error: 0.4212 - val_loss: 0.4641 - val_mean_absolute_error: 0.4641\n",
            "Epoch 480/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4319 - mean_absolute_error: 0.4319 - val_loss: 0.4709 - val_mean_absolute_error: 0.4709\n",
            "Epoch 481/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4241 - mean_absolute_error: 0.4241 - val_loss: 0.4663 - val_mean_absolute_error: 0.4663\n",
            "Epoch 482/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4229 - mean_absolute_error: 0.4229 - val_loss: 0.4672 - val_mean_absolute_error: 0.4672\n",
            "Epoch 483/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4256 - mean_absolute_error: 0.4256 - val_loss: 0.4637 - val_mean_absolute_error: 0.4637\n",
            "Epoch 484/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4268 - mean_absolute_error: 0.4268 - val_loss: 0.4675 - val_mean_absolute_error: 0.4675\n",
            "Epoch 485/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4260 - mean_absolute_error: 0.4260 - val_loss: 0.4717 - val_mean_absolute_error: 0.4717\n",
            "Epoch 486/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4260 - mean_absolute_error: 0.4260 - val_loss: 0.4641 - val_mean_absolute_error: 0.4641\n",
            "Epoch 487/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4254 - mean_absolute_error: 0.4254 - val_loss: 0.4624 - val_mean_absolute_error: 0.4624\n",
            "Epoch 488/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4193 - mean_absolute_error: 0.4193 - val_loss: 0.4634 - val_mean_absolute_error: 0.4634\n",
            "Epoch 489/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4200 - mean_absolute_error: 0.4200 - val_loss: 0.4649 - val_mean_absolute_error: 0.4649\n",
            "Epoch 490/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4212 - mean_absolute_error: 0.4212 - val_loss: 0.4649 - val_mean_absolute_error: 0.4649\n",
            "Epoch 491/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4290 - mean_absolute_error: 0.4290 - val_loss: 0.4691 - val_mean_absolute_error: 0.4691\n",
            "Epoch 492/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.4313 - mean_absolute_error: 0.4313 - val_loss: 0.4768 - val_mean_absolute_error: 0.4768\n",
            "Epoch 493/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4216 - mean_absolute_error: 0.4216 - val_loss: 0.4696 - val_mean_absolute_error: 0.4696\n",
            "Epoch 494/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4243 - mean_absolute_error: 0.4243 - val_loss: 0.4628 - val_mean_absolute_error: 0.4628\n",
            "Epoch 495/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4291 - mean_absolute_error: 0.4291 - val_loss: 0.4759 - val_mean_absolute_error: 0.4759\n",
            "Epoch 496/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4257 - mean_absolute_error: 0.4257 - val_loss: 0.4648 - val_mean_absolute_error: 0.4648\n",
            "Epoch 497/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4211 - mean_absolute_error: 0.4211 - val_loss: 0.4578 - val_mean_absolute_error: 0.4578\n",
            "Epoch 498/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4215 - mean_absolute_error: 0.4215 - val_loss: 0.4822 - val_mean_absolute_error: 0.4822\n",
            "Epoch 499/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4317 - mean_absolute_error: 0.4317 - val_loss: 0.4634 - val_mean_absolute_error: 0.4634\n",
            "Epoch 500/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4222 - mean_absolute_error: 0.4222 - val_loss: 0.4672 - val_mean_absolute_error: 0.4672\n"
          ]
        }
      ],
      "source": [
        "# history = dnn_neural.fit(X_train, np.array(Y_train), validation_split = 0.2,batch_size=32, epochs=5)\n",
        "history = NN_model.fit(X_train, np.array(Y_train), validation_split = 0.2,batch_size=32, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODK0Py842xLH",
        "outputId": "e6a773cf-7987-417d-c577-c114039ca85f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error'])"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ],
      "source": [
        "history.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "SMmlEUer2xLH",
        "outputId": "7e4a49be-27bf-46a9-eac1-bf5096e060b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = history.history['mean_absolute_error']\n",
        "loss_val = history.history['val_mean_absolute_error']\n",
        "epochs = range(1,len(loss_train)+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training rmse')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation rmse')\n",
        "plt.title('Training and Validation RMSE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm1ls8782xLH",
        "outputId": "d8e14abc-5df1-47d0-b33c-16c2200e209c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "NN predictions -> [2.0881295  5.982083   1.3506196  0.46821007 1.2475388  6.178735\n",
            " 0.09977883 0.09977883 0.85726476 2.1596076  5.6165843  0.36494443\n",
            " 4.601857   0.26589906 0.37823856 1.7764688  0.17242056 5.5549536\n",
            " 0.4972887  0.09977883 0.7381842  0.09977883 0.09977883 1.0827017\n",
            " 6.706588   2.8741832  3.5463333  2.846189   0.09977883 2.0817833\n",
            " 1.4290254  3.6720645  3.5954258  0.09977883 1.7718275  0.8541309\n",
            " 1.7924163  4.9252543  0.09977883 1.2037785  2.024378   1.0271807\n",
            " 1.1571496  4.9027123  1.9760303  5.1131873  0.8418202  0.5340255\n",
            " 4.0086555  0.55076826 2.7534528  0.09977883 0.09977883 1.500011\n",
            " 2.0887372  4.477001   6.35626    0.21993269 1.2629087  0.82647955\n",
            " 3.3089063  0.99051124 5.9469624  2.1170573  0.09977883 0.21297756\n",
            " 0.8171994  2.8104858  2.5098677  2.531191   4.637383   0.30357903\n",
            " 0.8893074  0.09977883 0.3639739  0.1455584  1.1112504  3.6202266\n",
            " 1.6006522  0.09977883 2.9238145  1.6957848  2.2511015  5.426469\n",
            " 6.492807   0.26539662 0.45317784 3.6290696  2.0127678  4.395945\n",
            " 2.1686552  1.762841   1.6338336  2.7538657  4.458125   4.660707\n",
            " 3.8510156  1.2507043  1.549962   0.4511571  5.122621   2.1026568\n",
            " 4.31488    0.09977883 6.893804   4.1718917  6.671741   4.757027\n",
            " 0.09977883 5.789554   3.7780495  3.0571015  2.5737157  0.09977883\n",
            " 2.0923746  4.777268   0.27420017 0.35710666 1.8229008  5.2490687\n",
            " 1.2722898  0.27253568 0.09977883 2.5095832  5.4455466  0.09977883\n",
            " 0.13506797 4.2869854  0.86827326 5.3328357  5.825274   0.95749813\n",
            " 1.2000041  5.4892     4.27144    2.979264   3.2660375  6.863317\n",
            " 1.0218229  0.09977883 1.3200579  4.332709   4.1078544  3.8169413\n",
            " 0.92667055 4.088709   5.273683   0.53619576 1.5856528  6.650471\n",
            " 5.3445344  0.4139006  3.822634   1.0988537  5.6486006  0.11958051\n",
            " 0.830057   6.064878   1.7462823  0.81827754 2.075047   5.246629\n",
            " 1.4739573  1.6398873  1.622611   3.829158   0.23767725 5.245795\n",
            " 0.10737346 0.71616715 1.0114597  4.5231557  1.8966296  0.40375155\n",
            " 0.90952474 2.1010592  2.2015018  1.5105574  0.09977883 3.0423403\n",
            " 0.49908906 1.5888264  2.5454116  1.12517    1.4400744  2.5764878\n",
            " 0.09977883 1.1638727  0.09977883 2.1001241  5.821004   1.5909913\n",
            " 0.18729798 5.1034083  0.3773022  1.3862612  0.09977883 6.6726046\n",
            " 3.0202916  1.7443159  4.1449914  1.9994283  0.09977883 4.323719\n",
            " 1.4352038  4.2194443  0.34246713 1.213346   4.307909   0.5138886\n",
            " 3.6848626  4.5490675  0.49333674 4.8106914  2.011635   4.39381\n",
            " 0.25812036 0.09977883 1.4096344  6.181131   0.09977883 1.1348774\n",
            " 0.3329847  0.22407863 3.7675307  0.09977883 4.145073   1.3728786\n",
            " 0.46596205 0.09977883 5.6731095  0.09977883 0.6461914  3.9253833\n",
            " 5.4622436  2.3068478  1.830292   5.774159   2.0028546  2.6808891\n",
            " 0.09977883 5.465511   1.3773499  1.2537117  0.09977883 1.3670483\n",
            " 4.905192   1.8384645  0.14646916 0.96986145 0.09977883 0.2411194\n",
            " 4.227003   1.6856055  1.4777715  6.799131   5.619591   0.32473028\n",
            " 4.3461103  4.8330994  3.6607287  3.531472   0.23265402 0.09977883\n",
            " 4.402326   5.0243845  3.9911911  1.6922786  0.3044116  0.78012776\n",
            " 1.1907325  0.09977883 0.79480684 0.09977883 0.4492429  2.5195582\n",
            " 1.380738   1.0737336  0.4459039  5.3940206  0.17269278 0.3440122\n",
            " 0.09977883 4.015914   2.181552   0.26520395 1.9934871  3.6348982\n",
            " 3.648369   1.1779904  5.012053   0.4440634  0.21898738 6.217961\n",
            " 0.09977883 0.205733   3.65091    1.6596887  6.8720894  6.1084085\n",
            " 1.6977062  1.0024549  5.841126   1.0871714  5.6685853  0.09977883\n",
            " 1.7927401  5.4479117  1.834429   0.5356033  0.09977883 1.282867\n",
            " 0.09977883 0.930802   0.09977883 3.567366   3.6747088  0.09977883\n",
            " 1.8495543 ]\n",
            "Real values\t  [1.5, 5.5, 1.5, 0.5, 1.5, 5.5, 0.1, 0.08, 1, 3.2, 5.5, 0.5, 3.5, 0.1, 0.5, 1.5, 0.09, 5.5, 0.5, 0.1, 1, 0.1, 0.4, 1, 6.6, 1.8, 3.5, 1.8, 0.1, 2]\n",
            " The Mean absolute percentage Error is -> [47.16099762] %\n"
          ]
        }
      ],
      "source": [
        "# Predict test set labels\n",
        "y_pred = NN_model.predict(X_test)\n",
        "\n",
        "print(\"NN predictions ->\",y_pred[:, 0])\n",
        "# print( \"Predicted values \", np.round( Y_pred[:30], 2 )/10 )\n",
        "\n",
        "\n",
        "\n",
        "print( \"Real values\t \", Y_test[:30]  )\n",
        "\n",
        "#print( \" Coefficients of the LR \",  model.coef_, model.intercept_ )\n",
        "\n",
        "\n",
        "\n",
        "mse = mean_absolute_percentage_error(Y_test, y_pred, multioutput=\"raw_values\") * 100 ##( |ypred - ytrue|/ytrue * 100)\n",
        "\n",
        "print(' The Mean absolute percentage Error is ->', mse, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "-LYv5H542xLH"
      },
      "outputs": [],
      "source": [
        "base_rmse = np.sqrt(mean_squared_error(Y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-srQa8K0mn2",
        "outputId": "9446b9c3-f01c-4fdb-f17d-8d366047514d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6212357765676298"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ],
      "source": [
        "base_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGc7Htu92xLH"
      },
      "source": [
        "CONVOLUTIONAL NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "UhxTRl-rZBat"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape( (len(X_train), 12, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "HVq6AiuXZBav"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.reshape( (len(X_test), 12, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVFSPEUQZBaw",
        "outputId": "bb5f6164-4afe-4185-8ad3-7955cc50ac29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1276, 12, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "qS4TSutQZBax"
      },
      "outputs": [],
      "source": [
        "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], np.array(Y_train).shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "uIx2KD1i2xLH"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=6, kernel_size=3, activation='softsign', input_shape=(n_timesteps,n_features)))\n",
        "model.add(Conv1D(filters=4, kernel_size=3, activation='softsign'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "# model.add(Dense(8, activation='relu'))\n",
        "# model.add(Dense(1, activation='softmax'))\n",
        "model.add(Dense(6,  kernel_initializer='normal', activation='softsign')),\n",
        "model.add(Dense(3,  kernel_initializer='normal', activation='softsign')),\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=rmse, optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdzEkVrN2xLH",
        "outputId": "240c2655-8515-4123-e95d-837657c83875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 2.6987 - val_loss: 2.7848\n",
            "Epoch 2/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6695 - val_loss: 2.7372\n",
            "Epoch 3/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.6216 - val_loss: 2.6887\n",
            "Epoch 4/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5777 - val_loss: 2.6525\n",
            "Epoch 5/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5544 - val_loss: 2.6258\n",
            "Epoch 6/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5354 - val_loss: 2.6053\n",
            "Epoch 7/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5044 - val_loss: 2.5889\n",
            "Epoch 8/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.5012 - val_loss: 2.5756\n",
            "Epoch 9/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4972 - val_loss: 2.5646\n",
            "Epoch 10/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4864 - val_loss: 2.5554\n",
            "Epoch 11/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4711 - val_loss: 2.5476\n",
            "Epoch 12/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4689 - val_loss: 2.5408\n",
            "Epoch 13/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4610 - val_loss: 2.5350\n",
            "Epoch 14/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4550 - val_loss: 2.5300\n",
            "Epoch 15/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4505 - val_loss: 2.5255\n",
            "Epoch 16/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4557 - val_loss: 2.5216\n",
            "Epoch 17/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4448 - val_loss: 2.5181\n",
            "Epoch 18/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4407 - val_loss: 2.5151\n",
            "Epoch 19/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4450 - val_loss: 2.5123\n",
            "Epoch 20/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4393 - val_loss: 2.5098\n",
            "Epoch 21/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4368 - val_loss: 2.5075\n",
            "Epoch 22/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4399 - val_loss: 2.5055\n",
            "Epoch 23/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4330 - val_loss: 2.5037\n",
            "Epoch 24/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4274 - val_loss: 2.5020\n",
            "Epoch 25/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4226 - val_loss: 2.5004\n",
            "Epoch 26/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4309 - val_loss: 2.4990\n",
            "Epoch 27/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4205 - val_loss: 2.4977\n",
            "Epoch 28/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4355 - val_loss: 2.4965\n",
            "Epoch 29/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4191 - val_loss: 2.4953\n",
            "Epoch 30/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4166 - val_loss: 2.4943\n",
            "Epoch 31/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4163 - val_loss: 2.4934\n",
            "Epoch 32/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4209 - val_loss: 2.4925\n",
            "Epoch 33/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4233 - val_loss: 2.4917\n",
            "Epoch 34/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4166 - val_loss: 2.4909\n",
            "Epoch 35/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4189 - val_loss: 2.4901\n",
            "Epoch 36/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4191 - val_loss: 2.4895\n",
            "Epoch 37/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4174 - val_loss: 2.4888\n",
            "Epoch 38/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4135 - val_loss: 2.4882\n",
            "Epoch 39/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4215 - val_loss: 2.4876\n",
            "Epoch 40/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4157 - val_loss: 2.4871\n",
            "Epoch 41/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4115 - val_loss: 2.4866\n",
            "Epoch 42/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3988 - val_loss: 2.4862\n",
            "Epoch 43/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4161 - val_loss: 2.4857\n",
            "Epoch 44/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4150 - val_loss: 2.4853\n",
            "Epoch 45/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4129 - val_loss: 2.4849\n",
            "Epoch 46/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4080 - val_loss: 2.4845\n",
            "Epoch 47/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4137 - val_loss: 2.4841\n",
            "Epoch 48/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4136 - val_loss: 2.4838\n",
            "Epoch 49/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4133 - val_loss: 2.4835\n",
            "Epoch 50/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4138 - val_loss: 2.4832\n",
            "Epoch 51/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4149 - val_loss: 2.4829\n",
            "Epoch 52/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4054 - val_loss: 2.4826\n",
            "Epoch 53/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4178 - val_loss: 2.4823\n",
            "Epoch 54/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3942 - val_loss: 2.4820\n",
            "Epoch 55/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4095 - val_loss: 2.4818\n",
            "Epoch 56/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4110 - val_loss: 2.4816\n",
            "Epoch 57/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4813\n",
            "Epoch 58/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4050 - val_loss: 2.4811\n",
            "Epoch 59/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4172 - val_loss: 2.4809\n",
            "Epoch 60/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4112 - val_loss: 2.4807\n",
            "Epoch 61/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4152 - val_loss: 2.4805\n",
            "Epoch 62/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4114 - val_loss: 2.4803\n",
            "Epoch 63/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4115 - val_loss: 2.4802\n",
            "Epoch 64/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4088 - val_loss: 2.4800\n",
            "Epoch 65/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4141 - val_loss: 2.4798\n",
            "Epoch 66/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4096 - val_loss: 2.4797\n",
            "Epoch 67/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4049 - val_loss: 2.4795\n",
            "Epoch 68/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4034 - val_loss: 2.4794\n",
            "Epoch 69/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4025 - val_loss: 2.4792\n",
            "Epoch 70/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4075 - val_loss: 2.4791\n",
            "Epoch 71/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4082 - val_loss: 2.4790\n",
            "Epoch 72/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4121 - val_loss: 2.4789\n",
            "Epoch 73/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4086 - val_loss: 2.4787\n",
            "Epoch 74/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4154 - val_loss: 2.4786\n",
            "Epoch 75/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4101 - val_loss: 2.4785\n",
            "Epoch 76/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4091 - val_loss: 2.4784\n",
            "Epoch 77/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4034 - val_loss: 2.4783\n",
            "Epoch 78/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4061 - val_loss: 2.4782\n",
            "Epoch 79/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4083 - val_loss: 2.4781\n",
            "Epoch 80/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4047 - val_loss: 2.4780\n",
            "Epoch 81/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3986 - val_loss: 2.4779\n",
            "Epoch 82/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4045 - val_loss: 2.4778\n",
            "Epoch 83/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4092 - val_loss: 2.4777\n",
            "Epoch 84/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4062 - val_loss: 2.4777\n",
            "Epoch 85/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4081 - val_loss: 2.4776\n",
            "Epoch 86/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4103 - val_loss: 2.4775\n",
            "Epoch 87/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3928 - val_loss: 2.4774\n",
            "Epoch 88/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4113 - val_loss: 2.4773\n",
            "Epoch 89/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4085 - val_loss: 2.4773\n",
            "Epoch 90/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4057 - val_loss: 2.4772\n",
            "Epoch 91/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4014 - val_loss: 2.4771\n",
            "Epoch 92/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4037 - val_loss: 2.4770\n",
            "Epoch 93/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4139 - val_loss: 2.4768\n",
            "Epoch 94/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4159 - val_loss: 2.4769\n",
            "Epoch 95/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4115 - val_loss: 2.4768\n",
            "Epoch 96/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4105 - val_loss: 2.4767\n",
            "Epoch 97/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3983 - val_loss: 2.4767\n",
            "Epoch 98/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4019 - val_loss: 2.4766\n",
            "Epoch 99/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4766\n",
            "Epoch 100/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4080 - val_loss: 2.4766\n",
            "Epoch 101/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4040 - val_loss: 2.4765\n",
            "Epoch 102/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4072 - val_loss: 2.4765\n",
            "Epoch 103/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4117 - val_loss: 2.4764\n",
            "Epoch 104/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3910 - val_loss: 2.4764\n",
            "Epoch 105/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4763\n",
            "Epoch 106/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4006 - val_loss: 2.4763\n",
            "Epoch 107/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3948 - val_loss: 2.4762\n",
            "Epoch 108/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3948 - val_loss: 2.4762\n",
            "Epoch 109/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4094 - val_loss: 2.4762\n",
            "Epoch 110/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4015 - val_loss: 2.4761\n",
            "Epoch 111/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4761\n",
            "Epoch 112/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4055 - val_loss: 2.4761\n",
            "Epoch 113/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4063 - val_loss: 2.4760\n",
            "Epoch 114/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4760\n",
            "Epoch 115/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4025 - val_loss: 2.4759\n",
            "Epoch 116/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4112 - val_loss: 2.4759\n",
            "Epoch 117/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4084 - val_loss: 2.4759\n",
            "Epoch 118/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4029 - val_loss: 2.4758\n",
            "Epoch 119/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3970 - val_loss: 2.4758\n",
            "Epoch 120/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4111 - val_loss: 2.4757\n",
            "Epoch 121/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4001 - val_loss: 2.4757\n",
            "Epoch 122/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4060 - val_loss: 2.4757\n",
            "Epoch 123/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3951 - val_loss: 2.4756\n",
            "Epoch 124/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4111 - val_loss: 2.4756\n",
            "Epoch 125/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3898 - val_loss: 2.4756\n",
            "Epoch 126/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4755\n",
            "Epoch 127/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4018 - val_loss: 2.4755\n",
            "Epoch 128/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4086 - val_loss: 2.4755\n",
            "Epoch 129/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4050 - val_loss: 2.4755\n",
            "Epoch 130/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4128 - val_loss: 2.4755\n",
            "Epoch 131/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4002 - val_loss: 2.4754\n",
            "Epoch 132/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4050 - val_loss: 2.4754\n",
            "Epoch 133/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4754\n",
            "Epoch 134/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3986 - val_loss: 2.4754\n",
            "Epoch 135/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4009 - val_loss: 2.4754\n",
            "Epoch 136/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4754\n",
            "Epoch 137/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4132 - val_loss: 2.4753\n",
            "Epoch 138/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4115 - val_loss: 2.4754\n",
            "Epoch 139/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4046 - val_loss: 2.4753\n",
            "Epoch 140/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4753\n",
            "Epoch 141/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3999 - val_loss: 2.4753\n",
            "Epoch 142/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4071 - val_loss: 2.4753\n",
            "Epoch 143/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4071 - val_loss: 2.4753\n",
            "Epoch 144/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4013 - val_loss: 2.4752\n",
            "Epoch 145/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4752\n",
            "Epoch 146/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4099 - val_loss: 2.4752\n",
            "Epoch 147/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4106 - val_loss: 2.4752\n",
            "Epoch 148/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4030 - val_loss: 2.4752\n",
            "Epoch 149/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4028 - val_loss: 2.4752\n",
            "Epoch 150/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4094 - val_loss: 2.4751\n",
            "Epoch 151/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4105 - val_loss: 2.4751\n",
            "Epoch 152/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4072 - val_loss: 2.4751\n",
            "Epoch 153/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4057 - val_loss: 2.4751\n",
            "Epoch 154/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4109 - val_loss: 2.4751\n",
            "Epoch 155/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4003 - val_loss: 2.4751\n",
            "Epoch 156/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4054 - val_loss: 2.4751\n",
            "Epoch 157/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4127 - val_loss: 2.4750\n",
            "Epoch 158/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4750\n",
            "Epoch 159/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4037 - val_loss: 2.4750\n",
            "Epoch 160/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4031 - val_loss: 2.4750\n",
            "Epoch 161/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3947 - val_loss: 2.4750\n",
            "Epoch 162/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4091 - val_loss: 2.4750\n",
            "Epoch 163/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4034 - val_loss: 2.4749\n",
            "Epoch 164/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4047 - val_loss: 2.4749\n",
            "Epoch 165/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4024 - val_loss: 2.4748\n",
            "Epoch 166/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4075 - val_loss: 2.4748\n",
            "Epoch 167/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4747\n",
            "Epoch 168/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4004 - val_loss: 2.4747\n",
            "Epoch 169/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4050 - val_loss: 2.4747\n",
            "Epoch 170/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4033 - val_loss: 2.4747\n",
            "Epoch 171/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4062 - val_loss: 2.4747\n",
            "Epoch 172/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4094 - val_loss: 2.4748\n",
            "Epoch 173/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4016 - val_loss: 2.4748\n",
            "Epoch 174/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4032 - val_loss: 2.4748\n",
            "Epoch 175/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4116 - val_loss: 2.4748\n",
            "Epoch 176/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4748\n",
            "Epoch 177/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3972 - val_loss: 2.4748\n",
            "Epoch 178/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4037 - val_loss: 2.4747\n",
            "Epoch 179/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4746\n",
            "Epoch 180/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4009 - val_loss: 2.4746\n",
            "Epoch 181/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4746\n",
            "Epoch 182/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3917 - val_loss: 2.4746\n",
            "Epoch 183/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4091 - val_loss: 2.4746\n",
            "Epoch 184/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4027 - val_loss: 2.4747\n",
            "Epoch 185/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4041 - val_loss: 2.4746\n",
            "Epoch 186/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4000 - val_loss: 2.4746\n",
            "Epoch 187/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4066 - val_loss: 2.4746\n",
            "Epoch 188/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4746\n",
            "Epoch 189/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3967 - val_loss: 2.4746\n",
            "Epoch 190/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3955 - val_loss: 2.4746\n",
            "Epoch 191/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3932 - val_loss: 2.4746\n",
            "Epoch 192/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4021 - val_loss: 2.4746\n",
            "Epoch 193/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4024 - val_loss: 2.4746\n",
            "Epoch 194/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3981 - val_loss: 2.4746\n",
            "Epoch 195/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4018 - val_loss: 2.4746\n",
            "Epoch 196/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4045 - val_loss: 2.4746\n",
            "Epoch 197/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4046 - val_loss: 2.4746\n",
            "Epoch 198/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3975 - val_loss: 2.4746\n",
            "Epoch 199/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3986 - val_loss: 2.4746\n",
            "Epoch 200/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4041 - val_loss: 2.4746\n",
            "Epoch 201/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4013 - val_loss: 2.4746\n",
            "Epoch 202/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4099 - val_loss: 2.4746\n",
            "Epoch 203/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3970 - val_loss: 2.4746\n",
            "Epoch 204/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4042 - val_loss: 2.4746\n",
            "Epoch 205/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4038 - val_loss: 2.4746\n",
            "Epoch 206/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3919 - val_loss: 2.4745\n",
            "Epoch 207/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4028 - val_loss: 2.4745\n",
            "Epoch 208/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3961 - val_loss: 2.4745\n",
            "Epoch 209/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4083 - val_loss: 2.4745\n",
            "Epoch 210/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 2.4745\n",
            "Epoch 211/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4098 - val_loss: 2.4745\n",
            "Epoch 212/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4024 - val_loss: 2.4745\n",
            "Epoch 213/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3970 - val_loss: 2.4745\n",
            "Epoch 214/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3941 - val_loss: 2.4745\n",
            "Epoch 215/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4069 - val_loss: 2.4745\n",
            "Epoch 216/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4009 - val_loss: 2.4745\n",
            "Epoch 217/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3874 - val_loss: 2.4745\n",
            "Epoch 218/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4745\n",
            "Epoch 219/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4109 - val_loss: 2.4745\n",
            "Epoch 220/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4745\n",
            "Epoch 221/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4092 - val_loss: 2.4745\n",
            "Epoch 222/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4014 - val_loss: 2.4745\n",
            "Epoch 223/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4745\n",
            "Epoch 224/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4011 - val_loss: 2.4745\n",
            "Epoch 225/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3959 - val_loss: 2.4744\n",
            "Epoch 226/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3944 - val_loss: 2.4744\n",
            "Epoch 227/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4063 - val_loss: 2.4744\n",
            "Epoch 228/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4082 - val_loss: 2.4744\n",
            "Epoch 229/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3979 - val_loss: 2.4744\n",
            "Epoch 230/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4130 - val_loss: 2.4744\n",
            "Epoch 231/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4744\n",
            "Epoch 232/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4108 - val_loss: 2.4744\n",
            "Epoch 233/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3999 - val_loss: 2.4743\n",
            "Epoch 234/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4043 - val_loss: 2.4744\n",
            "Epoch 235/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4100 - val_loss: 2.4744\n",
            "Epoch 236/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4023 - val_loss: 2.4744\n",
            "Epoch 237/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4024 - val_loss: 2.4744\n",
            "Epoch 238/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4744\n",
            "Epoch 239/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4034 - val_loss: 2.4744\n",
            "Epoch 240/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4147 - val_loss: 2.4744\n",
            "Epoch 241/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4020 - val_loss: 2.4744\n",
            "Epoch 242/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4009 - val_loss: 2.4744\n",
            "Epoch 243/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4101 - val_loss: 2.4744\n",
            "Epoch 244/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4047 - val_loss: 2.4744\n",
            "Epoch 245/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4016 - val_loss: 2.4744\n",
            "Epoch 246/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4040 - val_loss: 2.4744\n",
            "Epoch 247/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3933 - val_loss: 2.4744\n",
            "Epoch 248/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4045 - val_loss: 2.4744\n",
            "Epoch 249/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4071 - val_loss: 2.4744\n",
            "Epoch 250/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3987 - val_loss: 2.4744\n",
            "Epoch 251/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3968 - val_loss: 2.4744\n",
            "Epoch 252/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4056 - val_loss: 2.4744\n",
            "Epoch 253/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3961 - val_loss: 2.4744\n",
            "Epoch 254/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4062 - val_loss: 2.4744\n",
            "Epoch 255/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3992 - val_loss: 2.4744\n",
            "Epoch 256/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3993 - val_loss: 2.4744\n",
            "Epoch 257/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4088 - val_loss: 2.4744\n",
            "Epoch 258/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4006 - val_loss: 2.4744\n",
            "Epoch 259/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3993 - val_loss: 2.4744\n",
            "Epoch 260/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 2.4744\n",
            "Epoch 261/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4057 - val_loss: 2.4744\n",
            "Epoch 262/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4014 - val_loss: 2.4744\n",
            "Epoch 263/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3971 - val_loss: 2.4744\n",
            "Epoch 264/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4099 - val_loss: 2.4744\n",
            "Epoch 265/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4012 - val_loss: 2.4744\n",
            "Epoch 266/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4026 - val_loss: 2.4744\n",
            "Epoch 267/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4076 - val_loss: 2.4744\n",
            "Epoch 268/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3996 - val_loss: 2.4744\n",
            "Epoch 269/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4114 - val_loss: 2.4743\n",
            "Epoch 270/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4060 - val_loss: 2.4743\n",
            "Epoch 271/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4022 - val_loss: 2.4743\n",
            "Epoch 272/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3997 - val_loss: 2.4743\n",
            "Epoch 273/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4016 - val_loss: 2.4743\n",
            "Epoch 274/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4033 - val_loss: 2.4743\n",
            "Epoch 275/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4092 - val_loss: 2.4743\n",
            "Epoch 276/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4083 - val_loss: 2.4743\n",
            "Epoch 277/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4058 - val_loss: 2.4743\n",
            "Epoch 278/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4743\n",
            "Epoch 279/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4079 - val_loss: 2.4743\n",
            "Epoch 280/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4743\n",
            "Epoch 281/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4032 - val_loss: 2.4743\n",
            "Epoch 282/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4743\n",
            "Epoch 283/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3915 - val_loss: 2.4743\n",
            "Epoch 284/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4015 - val_loss: 2.4743\n",
            "Epoch 285/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4005 - val_loss: 2.4743\n",
            "Epoch 286/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4033 - val_loss: 2.4743\n",
            "Epoch 287/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4050 - val_loss: 2.4743\n",
            "Epoch 288/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4096 - val_loss: 2.4743\n",
            "Epoch 289/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4075 - val_loss: 2.4743\n",
            "Epoch 290/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4045 - val_loss: 2.4743\n",
            "Epoch 291/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4106 - val_loss: 2.4743\n",
            "Epoch 292/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4047 - val_loss: 2.4743\n",
            "Epoch 293/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4076 - val_loss: 2.4743\n",
            "Epoch 294/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4052 - val_loss: 2.4743\n",
            "Epoch 295/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4068 - val_loss: 2.4743\n",
            "Epoch 296/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4089 - val_loss: 2.4743\n",
            "Epoch 297/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4743\n",
            "Epoch 298/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4002 - val_loss: 2.4743\n",
            "Epoch 299/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4087 - val_loss: 2.4743\n",
            "Epoch 300/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4045 - val_loss: 2.4743\n",
            "Epoch 301/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4122 - val_loss: 2.4743\n",
            "Epoch 302/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4046 - val_loss: 2.4742\n",
            "Epoch 303/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3982 - val_loss: 2.4742\n",
            "Epoch 304/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4046 - val_loss: 2.4742\n",
            "Epoch 305/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3883 - val_loss: 2.4742\n",
            "Epoch 306/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3954 - val_loss: 2.4743\n",
            "Epoch 307/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4098 - val_loss: 2.4743\n",
            "Epoch 308/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4054 - val_loss: 2.4743\n",
            "Epoch 309/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4097 - val_loss: 2.4743\n",
            "Epoch 310/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3871 - val_loss: 2.4743\n",
            "Epoch 311/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4055 - val_loss: 2.4743\n",
            "Epoch 312/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4102 - val_loss: 2.4743\n",
            "Epoch 313/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4086 - val_loss: 2.4743\n",
            "Epoch 314/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3968 - val_loss: 2.4743\n",
            "Epoch 315/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4100 - val_loss: 2.4743\n",
            "Epoch 316/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4086 - val_loss: 2.4743\n",
            "Epoch 317/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4075 - val_loss: 2.4743\n",
            "Epoch 318/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3964 - val_loss: 2.4743\n",
            "Epoch 319/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4057 - val_loss: 2.4743\n",
            "Epoch 320/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4150 - val_loss: 2.4743\n",
            "Epoch 321/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4031 - val_loss: 2.4743\n",
            "Epoch 322/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4135 - val_loss: 2.4743\n",
            "Epoch 323/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4070 - val_loss: 2.4743\n",
            "Epoch 324/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4067 - val_loss: 2.4743\n",
            "Epoch 325/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4079 - val_loss: 2.4743\n",
            "Epoch 326/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4054 - val_loss: 2.4743\n",
            "Epoch 327/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4055 - val_loss: 2.4743\n",
            "Epoch 328/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4061 - val_loss: 2.4743\n",
            "Epoch 329/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4038 - val_loss: 2.4743\n",
            "Epoch 330/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3960 - val_loss: 2.4743\n",
            "Epoch 331/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4061 - val_loss: 2.4743\n",
            "Epoch 332/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4068 - val_loss: 2.4743\n",
            "Epoch 333/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3965 - val_loss: 2.4743\n",
            "Epoch 334/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4028 - val_loss: 2.4743\n",
            "Epoch 335/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4030 - val_loss: 2.4743\n",
            "Epoch 336/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4066 - val_loss: 2.4743\n",
            "Epoch 337/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3992 - val_loss: 2.4743\n",
            "Epoch 338/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4094 - val_loss: 2.4743\n",
            "Epoch 339/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4013 - val_loss: 2.4743\n",
            "Epoch 340/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4012 - val_loss: 2.4743\n",
            "Epoch 341/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4036 - val_loss: 2.4743\n",
            "Epoch 342/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4069 - val_loss: 2.4743\n",
            "Epoch 343/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4051 - val_loss: 2.4743\n",
            "Epoch 344/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3949 - val_loss: 2.4743\n",
            "Epoch 345/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4029 - val_loss: 2.4743\n",
            "Epoch 346/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4030 - val_loss: 2.4743\n",
            "Epoch 347/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4060 - val_loss: 2.4743\n",
            "Epoch 348/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4076 - val_loss: 2.4743\n",
            "Epoch 349/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3948 - val_loss: 2.4743\n",
            "Epoch 350/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3899 - val_loss: 2.4743\n",
            "Epoch 351/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3981 - val_loss: 2.4743\n",
            "Epoch 352/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3997 - val_loss: 2.4743\n",
            "Epoch 353/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3932 - val_loss: 2.4743\n",
            "Epoch 354/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4053 - val_loss: 2.4743\n",
            "Epoch 355/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3960 - val_loss: 2.4741\n",
            "Epoch 356/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4048 - val_loss: 2.4743\n",
            "Epoch 357/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4031 - val_loss: 2.4743\n",
            "Epoch 358/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4047 - val_loss: 2.4743\n",
            "Epoch 359/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4043 - val_loss: 2.4743\n",
            "Epoch 360/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.4155 - val_loss: 2.4743\n",
            "Epoch 361/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.4066 - val_loss: 2.4743\n",
            "Epoch 362/500\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.4076 - val_loss: 2.4743\n",
            "Epoch 363/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4072 - val_loss: 2.4743\n",
            "Epoch 364/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3984 - val_loss: 2.4743\n",
            "Epoch 365/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3931 - val_loss: 2.4742\n",
            "Epoch 366/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3976 - val_loss: 2.4741\n",
            "Epoch 367/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4038 - val_loss: 2.4741\n",
            "Epoch 368/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4023 - val_loss: 2.4741\n",
            "Epoch 369/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3979 - val_loss: 2.4741\n",
            "Epoch 370/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4741\n",
            "Epoch 371/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4079 - val_loss: 2.4741\n",
            "Epoch 372/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4029 - val_loss: 2.4741\n",
            "Epoch 373/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4048 - val_loss: 2.4741\n",
            "Epoch 374/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4011 - val_loss: 2.4741\n",
            "Epoch 375/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4061 - val_loss: 2.4742\n",
            "Epoch 376/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4047 - val_loss: 2.4743\n",
            "Epoch 377/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4741\n",
            "Epoch 378/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4043 - val_loss: 2.4741\n",
            "Epoch 379/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4006 - val_loss: 2.4741\n",
            "Epoch 380/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4062 - val_loss: 2.4741\n",
            "Epoch 381/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4742\n",
            "Epoch 382/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4076 - val_loss: 2.4742\n",
            "Epoch 383/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4056 - val_loss: 2.4743\n",
            "Epoch 384/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3914 - val_loss: 2.4743\n",
            "Epoch 385/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3970 - val_loss: 2.4743\n",
            "Epoch 386/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4016 - val_loss: 2.4743\n",
            "Epoch 387/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4058 - val_loss: 2.4741\n",
            "Epoch 388/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4081 - val_loss: 2.4741\n",
            "Epoch 389/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3983 - val_loss: 2.4741\n",
            "Epoch 390/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4060 - val_loss: 2.4743\n",
            "Epoch 391/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4044 - val_loss: 2.4743\n",
            "Epoch 392/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4743\n",
            "Epoch 393/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3946 - val_loss: 2.4743\n",
            "Epoch 394/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3997 - val_loss: 2.4742\n",
            "Epoch 395/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3985 - val_loss: 2.4743\n",
            "Epoch 396/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3947 - val_loss: 2.4743\n",
            "Epoch 397/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4070 - val_loss: 2.4743\n",
            "Epoch 398/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4743\n",
            "Epoch 399/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3992 - val_loss: 2.4743\n",
            "Epoch 400/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4094 - val_loss: 2.4743\n",
            "Epoch 401/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4002 - val_loss: 2.4743\n",
            "Epoch 402/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3897 - val_loss: 2.4743\n",
            "Epoch 403/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4024 - val_loss: 2.4743\n",
            "Epoch 404/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3878 - val_loss: 2.4743\n",
            "Epoch 405/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4109 - val_loss: 2.4743\n",
            "Epoch 406/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4063 - val_loss: 2.4743\n",
            "Epoch 407/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4070 - val_loss: 2.4743\n",
            "Epoch 408/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4053 - val_loss: 2.4743\n",
            "Epoch 409/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3994 - val_loss: 2.4743\n",
            "Epoch 410/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4081 - val_loss: 2.4743\n",
            "Epoch 411/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3953 - val_loss: 2.4743\n",
            "Epoch 412/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3950 - val_loss: 2.4743\n",
            "Epoch 413/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4021 - val_loss: 2.4743\n",
            "Epoch 414/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3966 - val_loss: 2.4743\n",
            "Epoch 415/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3972 - val_loss: 2.4743\n",
            "Epoch 416/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3985 - val_loss: 2.4743\n",
            "Epoch 417/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 2.4743\n",
            "Epoch 418/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4006 - val_loss: 2.4743\n",
            "Epoch 419/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3966 - val_loss: 2.4743\n",
            "Epoch 420/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3965 - val_loss: 2.4743\n",
            "Epoch 421/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4066 - val_loss: 2.4743\n",
            "Epoch 422/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4062 - val_loss: 2.4743\n",
            "Epoch 423/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4097 - val_loss: 2.4743\n",
            "Epoch 424/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4060 - val_loss: 2.4743\n",
            "Epoch 425/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3906 - val_loss: 2.4743\n",
            "Epoch 426/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4019 - val_loss: 2.4743\n",
            "Epoch 427/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4041 - val_loss: 2.4743\n",
            "Epoch 428/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4005 - val_loss: 2.4743\n",
            "Epoch 429/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4111 - val_loss: 2.4743\n",
            "Epoch 430/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4743\n",
            "Epoch 431/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4057 - val_loss: 2.4743\n",
            "Epoch 432/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4027 - val_loss: 2.4743\n",
            "Epoch 433/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4052 - val_loss: 2.4743\n",
            "Epoch 434/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3993 - val_loss: 2.4743\n",
            "Epoch 435/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3984 - val_loss: 2.4743\n",
            "Epoch 436/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3988 - val_loss: 2.4743\n",
            "Epoch 437/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4086 - val_loss: 2.4743\n",
            "Epoch 438/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4016 - val_loss: 2.4743\n",
            "Epoch 439/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4099 - val_loss: 2.4743\n",
            "Epoch 440/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3951 - val_loss: 2.4743\n",
            "Epoch 441/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4100 - val_loss: 2.4743\n",
            "Epoch 442/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4010 - val_loss: 2.4743\n",
            "Epoch 443/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4088 - val_loss: 2.4743\n",
            "Epoch 444/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4058 - val_loss: 2.4743\n",
            "Epoch 445/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4080 - val_loss: 2.4743\n",
            "Epoch 446/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4058 - val_loss: 2.4743\n",
            "Epoch 447/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4082 - val_loss: 2.4743\n",
            "Epoch 448/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4743\n",
            "Epoch 449/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4106 - val_loss: 2.4743\n",
            "Epoch 450/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4005 - val_loss: 2.4743\n",
            "Epoch 451/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4069 - val_loss: 2.4743\n",
            "Epoch 452/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4066 - val_loss: 2.4743\n",
            "Epoch 453/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3979 - val_loss: 2.4743\n",
            "Epoch 454/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4082 - val_loss: 2.4743\n",
            "Epoch 455/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4743\n",
            "Epoch 456/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 2.4743\n",
            "Epoch 457/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4006 - val_loss: 2.4743\n",
            "Epoch 458/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3996 - val_loss: 2.4743\n",
            "Epoch 459/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4073 - val_loss: 2.4743\n",
            "Epoch 460/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4083 - val_loss: 2.4743\n",
            "Epoch 461/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4091 - val_loss: 2.4743\n",
            "Epoch 462/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4039 - val_loss: 2.4743\n",
            "Epoch 463/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3979 - val_loss: 2.4743\n",
            "Epoch 464/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4004 - val_loss: 2.4743\n",
            "Epoch 465/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4743\n",
            "Epoch 466/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4116 - val_loss: 2.4743\n",
            "Epoch 467/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4028 - val_loss: 2.4743\n",
            "Epoch 468/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4743\n",
            "Epoch 469/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3990 - val_loss: 2.4743\n",
            "Epoch 470/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4059 - val_loss: 2.4743\n",
            "Epoch 471/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4051 - val_loss: 2.4743\n",
            "Epoch 472/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4106 - val_loss: 2.4743\n",
            "Epoch 473/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4130 - val_loss: 2.4743\n",
            "Epoch 474/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3948 - val_loss: 2.4743\n",
            "Epoch 475/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3944 - val_loss: 2.4743\n",
            "Epoch 476/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4076 - val_loss: 2.4743\n",
            "Epoch 477/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4022 - val_loss: 2.4743\n",
            "Epoch 478/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4015 - val_loss: 2.4743\n",
            "Epoch 479/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4055 - val_loss: 2.4743\n",
            "Epoch 480/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4022 - val_loss: 2.4743\n",
            "Epoch 481/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4088 - val_loss: 2.4743\n",
            "Epoch 482/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3960 - val_loss: 2.4743\n",
            "Epoch 483/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4126 - val_loss: 2.4743\n",
            "Epoch 484/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3901 - val_loss: 2.4743\n",
            "Epoch 485/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4019 - val_loss: 2.4743\n",
            "Epoch 486/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3950 - val_loss: 2.4743\n",
            "Epoch 487/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4059 - val_loss: 2.4743\n",
            "Epoch 488/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4037 - val_loss: 2.4743\n",
            "Epoch 489/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4035 - val_loss: 2.4743\n",
            "Epoch 490/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3945 - val_loss: 2.4743\n",
            "Epoch 491/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4075 - val_loss: 2.4743\n",
            "Epoch 492/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4077 - val_loss: 2.4743\n",
            "Epoch 493/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4091 - val_loss: 2.4743\n",
            "Epoch 494/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4029 - val_loss: 2.4743\n",
            "Epoch 495/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4035 - val_loss: 2.4743\n",
            "Epoch 496/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3958 - val_loss: 2.4743\n",
            "Epoch 497/500\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4053 - val_loss: 2.4743\n",
            "Epoch 498/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.4031 - val_loss: 2.4743\n",
            "Epoch 499/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3955 - val_loss: 2.4743\n",
            "Epoch 500/500\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3980 - val_loss: 2.4743\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, np.array(Y_train), validation_split = 0.2, epochs=500,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "_0ig0HOM2xLH",
        "outputId": "75d2618b-1944-4194-c78e-d12d0eaabfcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gU5fbA8e9J2wAJJRB6CSjSSyBSRCmiiAVsICig2EAudr2238XeLyJ6kYsoigpXRYpiQURpgooU6UWpEook1AQIIcn5/TG7m91kU4BsAng+z7NPZmfemTm7SebM+74z74iqYowxxuQUUtIBGGOMOT1ZgjDGGBOQJQhjjDEBWYIwxhgTkCUIY4wxAVmCMMYYE5AlCFPkRGSGiNxS1GVLkohsFZFLgrDduSJyh3u6n4h8V5iyJ7Gf2iKSKiKhJxur+fuxBGEAcB88PK8sETnq877fiWxLVS9X1Q+KuuzpSEQeE5H5AeZXEpF0EWla2G2p6kRV7VZEcfklNFX9U1WjVDWzKLafY18qIntEJMxnXrh7Xq4brURkvIhkiEi1HPOfFpHj7r+5AyLyk4i0dy8bKCKZOf5OU0WkelF/HpPNEoQBwH3wiFLVKOBPoIfPvImecr4HAQPABOACEambY35fYJWqri6BmErCfuByn/eXu+f5EZEywPXAQaB/gO186v4bjAUWAFNFRNzLfvb9O3W/dhbppzB+LEGYfIlIZxFJFJFHRWQ38L6IVBCRr0QkSUT2u6dr+qzj22wyUEQWiMhwd9ktInL5SZatKyLzRSRFRL4XkbdEZEIecRcmxudEZKF7e9+JSCWf5QNEZJuI7BWR/8vr+1HVRGA2MCDHopuBDwuKI0fMA0Vkgc/7S0VkvYgcFJFRgPgsO0dEZrvjSxaRiSJS3r3sI6A28KX7LPsREYlzn+mHuctUF5HpIrJPRDaKyJ0+235aRCaJyIfu72aNiCTk9R24feT+zH6fP0C564EDwLNAnk2Lqnoc+ACoClQsYN8mSCxBmMKoCsQAdYBBOH8377vf1waOAqPyWb8tsAGoBLwKjPM5KzyRsv8DfsU5YDxN7oOyr8LEeBNwK1AZiAAeBhCRxsB/3duv7t5fwIO62we+sYhIA6ClO94T/a4826gETAX+hfNdbAI6+BYBXnLH1wiohfOdoKoD8K8FvhpgF58Aie71ewEvisjFPst7usuUB6YXIubPgY4iUl5EKgAXAV8EKHcL8LF72w1FpHUen98FDAS2q2pyAfs2QWIJwhRGFvCUqh5T1aOquldVp6jqEVVNAV4AOuWz/jZVfcfd/v0BUA2ociJlRaQ2cD7wpKqmq+oCnANXQIWM8X1V/V1VjwKTcA7q4Bwwv1LV+ap6DBjm/g7yMs0d4wXu9zcDM1Q16SS+K48rgDWqOtl9Nj0S2O3z+Taq6iz37yQJGFHI7SIitXCSzaOqmqaqy4F38a8BLFDVb9y/h4+AFgVsNg34Eujjfk13z/Pdb22gC/A/Vf0L+CHHPgFuEJEDwHagNXCtz7J27r4Jz2tTYT6vOXmWIExhJKmq959dREqLyNvuJphDwHygvOR9hYzvge2IezLqBMtWB/b5zAPnIBJQIWPc7TN9xCem6r7bVtXDwN689uWO6TPgZndtpx/u5pWT+K48csagvu9FpIqIfCIiO9zbnYBT0ygMz3eZ4jNvG1DD533O7yZSCu5/+hDngJ9X89IAYJ07IQFMBG4SkXCfMpNUtbyqVlbVi1V1qc+yX9zLPK9zCojHnCJLEKYwcl6J8hDQAGirqmWBju75eTUbFYVdQIyIlPaZVyuf8qcS4y7fbbv3WVA7+AfADcClQDTO2fSpxJEzBsH/876I83tp5t5u/xzbzG+Y5p0432W0z7zawI4CYirIj2TXDhcEWH4zUE9EdovTnzUCJ6ldcYr7NUFiCcKcjGictvQDIhIDPBXsHarqNmAJ8LSIRIhz+WOPIMU4GbhKRC4UkQicDtWC/ld+xOl8HQt8oqrppxjH10ATEbnOfeZ+L05fkEc0kAocFJEawD9zrP8XUC/QhlV1O/AT8JKIRIpIc+B2nFrISXPXcnoAPTXHcwTcv69zgDY4TXktgaY4/TQ5m5nMacIShDkZI4FSQDLwC/BtMe23H9Aep7nneeBT4FgeZU86RlVdAwzFOXjtwrlcM7GAdRSnWaUO/s0rJxWHu2O2N/AyzuetDyz0KfIM0ArnctGvcTq0fb0E/MvdVv9wgF3cCMTh1Cam4fQxfV+Y2AqIe437+8vpFuALVV2lqrs9L+ANnGQcU4jNt5fc90Gcf6oxm7yJPTDInKlE5FNgvaoGvQZjzN+R1SDMGUNEzndf/x8iIt2Bq3EurzTGBIHdFWvOJFVxmlIq4jT5DFHV30o2JGPOXtbEZIwxJiBrYjLGGBNQ0JqY3HdrfohzTbQCY1X1jRxlyuFcWlfbHctwVX3fvewWnGEGAJ4vzIiflSpV0ri4uCL7DMYYc7ZbunRpsqrGBloWtCYmcYbyraaqy9w35CwFrlHVtT5lngDKqeqjIhKLMwZPVZw7WpcACTjJZSnQWlVzjQ7pKyEhQZcsWRKUz2OMMWcjEVmqqgEHYwxaE5Oq7lLVZe7pFGAd/rfyg3Pwj3bfJRoF7AMygMuAWaq6z50UZgHdgxWrMcaY3IqlD0JE4oB4YFGORaNwRqLcCawC7lPVLJxE4jvOTiK5k4tn24NEZImILElKSiriyI0x5u8r6AlCRKKAKcD9qnoox+LLgOU4g4e1BEaJSNkT2b6qjlXVBFVNiI0N2IxmjDHmJAT1Pgj3KI1TgImqmnMoAHDG4n/ZPUzBRhHZAjTEGTSss0+5msDcYMZqjMnf8ePHSUxMJC0treDC5rQTGRlJzZo1CQ8PL7iwWzCvYhJgHM7wviPyKPYn0BX4UUSq4Ix6uRnYiPMAkwruct2Ax4MVqzGmYImJiURHRxMXF0fez3sypyNVZe/evSQmJlK3bs6n4+YtmDWIDjjjv68SEc/470/gXNKKqo4BngPGi8gqnKGKH/U8PUpEngMWu9d7VlX3BTFWY0wB0tLSLDmcoUSEihUrcqL9tEFLEO4nfuX7l+R+4Hi3PJa9B7wXhNCMMSfJksOZ62R+d3YnNfDcczBzZklHYYwxpxdLEMArr8CsWSUdhTEmP3v37qVly5a0bNmSqlWrUqNGDe/79PT0fNddsmQJ9957b4H7uOCCCwos83dio7kCERFwLK/HzhhjTgsVK1Zk+XKnO/Ppp58mKiqKhx/OfhZSRkYGYWGBD2kJCQkkJAS8WdjPTz/9VDTB+sgvrtOd1SAAl8sShDFnooEDB3LXXXfRtm1bHnnkEX799Vfat29PfHw8F1xwARs2bABg7ty5XHXVVYCTXG677TY6d+5MvXr1ePPNN73bi4qK8pbv3LkzvXr1omHDhvTr1w/PsETffPMNDRs2pHXr1tx7773e7foaP348PXv25OKLL6Zr166MHz+ea665hksvvZS4uDhGjRrFiBEjiI+Pp127duzb51yD8+abb9K4cWOaN29O3759ATh8+DC33XYbbdq0IT4+ni+++CJ4X2gOZ2ZaK2IREVBADdUY4+P+b+9n+e7lBRc8AS2rtmRk95EnvF5iYiI//fQToaGhHDp0iB9//JGwsDC+//57nnjiCaZMmZJrnfXr1zNnzhxSUlJo0KABQ4YMyXV/wG+//caaNWuoXr06HTp0YOHChSQkJDB48GDmz59P3bp1ufHGG/OMa9myZaxcuZKYmBjGjx/P6tWr+e2330hLS+Pcc8/llVde4bfffuOBBx7gww8/5P777+fll19my5YtuFwuDhw4AMALL7zAxRdfzHvvvceBAwdo06YNl1xyCWXKlDnh7+pEWQ0Cq0EYcybr3bs3oaGhABw8eJDevXvTtGlTHnjgAdasCfR4bLjyyitxuVxUqlSJypUr89dff+Uq06ZNG2rWrElISAgtW7Zk69atrF+/nnr16nnvJcgvQVx66aXExGQ/artLly5ER0cTGxtLuXLl6NGjBwDNmjVj69atADRv3px+/foxYcIEb7PUd999x8svv0zLli3p3LkzaWlp/Pnnnyf+RZ0Eq0FgNQhjTtTJnOkHi++Z9LBhw+jSpQvTpk1j69atdO7cOeA6LpfLOx0aGkpGRsZJlSlsXDm3FxIS4n0fEhLi3fbXX3/N/Pnz+fLLL3nhhRdYtWoVqsqUKVNo0KDBCe2/KFgNAqtBGHO2OHjwIDVqOON6jh8/vsi336BBAzZv3uw94//000+LbNtZWVls376dLl268Morr3Dw4EFSU1O57LLL+M9//uPtA/ntt+J7yq4lCKwGYczZ4pFHHuHxxx8nPj7+hM/4C6NUqVKMHj2a7t2707p1a6KjoylXrlyRbDszM5P+/fvTrFkz4uPjuffeeylfvjzDhg3j+PHjNG/enCZNmjBs2LAi2V9hnFXPpD7ZBwZ17gyqMG9e0cdkzNli3bp1NGrUqKTDKHGpqalERUWhqgwdOpT69evzwAMPlHRYhRLod1giDww6k1gNwhhTWO+88w4tW7akSZMmHDx4kMGDB5d0SEFjndQ4fRDJySUdhTHmTPDAAw+cMTWGU2U1CJwEYTUIY4zxZwkCG2rDGGMCsQSB1SCMMSaQYD5RrhbwIVAFUGCsqr6Ro8w/gX4+sTQCYlV1n4hsBVKATCAjr172omA1CGOMyS2YNYgM4CFVbQy0A4aKSGPfAqr6b1VtqaotcR4pOi/Hk+O6uJcHLTmA1SCMOVt5Bt/buXMnvXr1Climc+fOFHR5/MiRIzly5Ij3/RVXXOEdK+lsFrQEoaq7VHWZezoFWAfUyGeVG4GPgxVPfqwGYczZrXr16kyePPmk18+ZIL755hvKly9fFKF5BePGvlNVLH0QIhIHxAOL8lheGugO+A67qMB3IrJURAYFMz4basOY099jjz3GW2+95X3/9NNPM3z4cFJTU+natSutWrWiWbNmAYfD3rp1K02bNgXg6NGj9O3bl0aNGnHttddy9OhRb7khQ4aQkJBAkyZNeOqppwBnCO6dO3fSpUsXunTpAkBcXBzJ7mvjR4wYQdOmTWnatCkjR4707q9Ro0bceeedNGnShG7duvntxyPncOUDBw5kyJAhtGvXjnr16jF37lxuu+02GjVqxMCBAwHnjuuBAwfStGlTmjVrxuuvvw7Apk2bvHd4X3TRRaxfv/5Uv/Lg3wchIlE4B/77VfVQHsV6AAtzNC9dqKo7RKQyMEtE1qvq/ADbHwQMAqhdu/ZJxRgRAZmZzss9KKQxJh/33w/Li3a0b1q2hJH5jAHYp08f7r//foYOHQrApEmTmDlzJpGRkUybNo2yZcuSnJxMu3bt6NmzZ57PYP7vf/9L6dKlWbduHStXrqRVq1beZS+88AIxMTFkZmbStWtXVq5cyb333suIESOYM2cOlSpV8tvW0qVLef/991m0aBGqStu2benUqRMVKlTgjz/+4OOPP+add97hhhtuYMqUKfTv3z9XPL7DlQ8cOJD9+/fz888/M336dHr27MnChQt59913Of/881m+fDmZmZns2LGD1atXA3ibugYNGsSYMWOoX78+ixYt4h//+AezZ88+od9BTkGtQYhIOE5ymKiqU/Mp2pcczUuqusP9cw8wDWgTaEVVHauqCaqaEBsbe1JxegZZtH4IY05f8fHx7Nmzh507d7JixQoqVKhArVq1UFWeeOIJmjdvziWXXMKOHTsCDt/tMX/+fO+Bunnz5jRv3ty7bNKkSbRq1Yr4+HjWrFnD2rVr841pwYIFXHvttZQpU4aoqCiuu+46fvzxRwDq1q1Ly5YtAWjdurV3gL+cfIcrB+jRowciQrNmzahSpQrNmjUjJCSEJk2asHXrVurVq8fmzZu55557+Pbbbylbtiypqan89NNP9O7dm5YtWzJ48GB27dpVqO81P8G8ikmAccA6VR2RT7lyQCegv8+8MkCIqqa4p7sBzwYr1ogI52d6OpQqFay9GHP2yO9MP5h69+7N5MmT2b17N3369AFg4sSJJCUlsXTpUsLDw4mLiyMtLe2Et71lyxaGDx/O4sWLqVChAgMHDjyp7XjkHC48UBMT5D0suO+Q4J73GRkZVKhQgRUrVjBz5kzGjBnDpEmTGDlyJOXLl/c+krWoBLMG0QEYAFwsIsvdrytE5C4Rucun3LXAd6p62GdeFWCBiKwAfgW+VtVvgxWo53dg/RDGnN769OnDJ598wuTJk+nduzfgDPFduXJlwsPDmTNnDtu2bct3Gx07duR///sfAKtXr2blypUAHDp0iDJlylCuXDn++usvZsyY4V0nOjqalJSUXNu66KKL+Pzzzzly5AiHDx9m2rRpXHTRRUX1cQNKTk4mKyuL66+/nueff55ly5ZRtmxZ6taty2effQaAqrJixYpT3lfQahCqugAI3AjoX248MD7HvM1Ai6AEFoBvDcIYc/pq0qQJKSkp1KhRg2rVqgHQr18/evToQbNmzUhISKBhw4b5bmPIkCHceuutNGrUiEaNGtG6dWsAWrRoQXx8PA0bNqRWrVp06NDBu86gQYPo3r071atXZ86cOd75rVq1YuDAgbRp47SA33HHHcTHx+fZnFQUduzYwa233kpWVhYAL730EuDUpIYMGcLzzz/P8ePH6du3Ly1anNph1Ib7Bj78EG65BTZuhHPOCUJgxpwFbLjvM58N930SrJPaGGNyswRBdhOT9UEYY0w2SxBYDcKYwjqbmqT/bk7md2cJAqtBGFMYkZGR7N2715LEGUhV2bt3L5GRkSe0nj1RDqtBGFMYNWvWJDExkaSkpJIOxZyEyMhIataseULrWILAahDGFEZ4eDh169Yt6TBMMbImJuxGOWOMCcQSBHajnDHGBGIJAqtBGGNMIJYgsBqEMcYEYgkCq0EYY0wgliCwGoQxxgRiCQKrQRhjTCCWILAahDHGBGIJAuc51KGhVoMwxhhfQUsQIlJLROaIyFoRWSMi9wUo80+fp82tFpFMEYlxL+suIhtEZKOIPBasOD1cLqtBGGOMr2DWIDKAh1S1MdAOGCoijX0LqOq/VbWlqrYEHgfmqeo+EQkF3gIuBxoDN+Zct6hFRFgNwhhjfAUtQajqLlVd5p5OAdYBNfJZ5UbgY/d0G2Cjqm5W1XTgE+DqYMUKVoMwxpiciqUPQkTigHhgUR7LSwPdgSnuWTWA7T5FEskjuYjIIBFZIiJLTnaUyR4f9yCdVKtBGGOMj6AnCBGJwjnw36+qh/Io1gNYqKr7TnT7qjpWVRNUNSE2NvakYpy/bT5ZoUetBmGMMT6CmiBEJBwnOUxU1an5FO1LdvMSwA6gls/7mu55QVEmvAwSetxqEMYY4yOYVzEJMA5Yp6oj8ilXDugEfOEzezFQX0TqikgETgKZHqxYS4eXRsKOWYIwxhgfwXxgUAdgALBKRJa75z0B1AZQ1THuedcC36nqYc+KqpohIncDM4FQ4D1VXROsQMtElGFfaLo1MRljjI+gJQhVXQBIIcqNB8YHmP8N8E2RBxZAmfAyaKjVIIwxxpfdSY1Tg9DQNEsQxhjjwxIETh+Ehh3m6NGSjsQYY04fliBwmpiyQi1BGGOML0sQOAkiM/QwaWklHYkxxpw+LEHg9EFkhKZaDcIYY3xYgsDpg8gISeHoUS3pUIwx5rRhCQL3Za5h1sRkjDG+LEHgNDERlkZGhpCRUdLRGGPM6cESBE4NgjCnA8L6IYwxxmEJAqcPgnAnM1gzkzHGOCxBAK4wF4Q5mcFqEMYY47AEAbhCXdbEZIwxOViCwF2DsCYmY4zxYwkCq0EYY0wgliCwPghjjAnEEgTuGoQ1MRljjJ9gPnK0lojMEZG1IrJGRO7Lo1xnEVnuLjPPZ/5WEVnlXrYkWHGCpwZhTUzGGOMrmI8czQAeUtVlIhINLBWRWaq61lNARMoDo4HuqvqniFTOsY0uqpocxBgBiAiNsCYmY4zJIWg1CFXdparL3NMpwDqgRo5iNwFTVfVPd7k9wYonP65QF0Q4j8Q+cqQkIjDGmNNPsfRBiEgcEA8syrHoPKCCiMwVkaUicrPPMgW+c88flM+2B4nIEhFZkpSUdFLxucJcEJEKwOHDJ7UJY4w56wSziQkAEYkCpgD3q+qhAPtvDXQFSgE/i8gvqvo7cKGq7nA3O80SkfWqOj/n9lV1LDAWICEh4aTG63Y6qZ3MkJp6MlswxpizT1BrECISjpMcJqrq1ABFEoGZqnrY3dcwH2gBoKo73D/3ANOANsGK0xXmgtBMwiIyLEEYY4xbMK9iEmAcsE5VR+RR7AvgQhEJE5HSQFtgnYiUcXdsIyJlgG7A6mDFGhEaAUB4ZLolCGOMcQtmE1MHYACwSkSWu+c9AdQGUNUxqrpORL4FVgJZwLuqulpE6gHTnBxDGPA/Vf02WIGGSAjhIeGERx7j8OHSwdqNMcacUYKWIFR1ASCFKPdv4N855m3G3dRUXCJCIwiPPGY1CGOMcbM7qd1cYS5CI9MsQRhjjFvQr2I6U7hCXUjkUUsQxhjjZjUIN1eYi5AISxDGGONhCcLNFeoixHXEbpQzxhg3SxBurjAX4jpiNQhjjHGzBOHmCnUhEYctQRhjjJslCLeI0AhwpZKaCllZJR2NMcaUPEsQbq4wF7gOomrjMRljDFiC8HKFulDXAQAOHizhYIwx5jRgCcLNFeZCXfsBOHCghIMxxpjTgCUIN1eoiyx3grAahDHGWILwcoW5yIzYB1iCMMYYsATh5Qp1kRHhPP7ampiMMcYShFdEaATHw50EYTUIY4yxBOHlCnWRHuY809oShDHGFJAgRORin+m6OZZdF6ygSoIrzEV6yCFcLmtiMsYYKLgGMdxnekqOZf/Kb0URqSUic0RkrYisEZH78ijXWUSWu8vM85nfXUQ2iMhGEXmsgDhPmSvURUZWBuXLqyUIY4yh4OdBSB7Tgd7nlAE8pKrL3M+XXiois1R1rXcDIuWB0UB3Vf1TRCq754cCbwGXAonAYhGZ7rtuUXOFuQCoEKPs3Vvgg/CMMeasV1ANQvOYDvTef6HqLlVd5p5OAdYBNXIUuwmYqqp/usvtcc9vA2xU1c2qmg58AlxdQKynxBXqThAVsti7N5h7MsaYM0NBNYh6IjIdp7bgmcb9vm7eq/kTkTggHliUY9F5QLiIzAWigTdU9UOcRLLdp1wi0DaPbQ8CBgHUrl27sCHl4qlBlIvJZMc2e9CeMcYUdCT0PWsfnmNZzvcBiUgUTv/F/ap6KMD+WwNdgVLAzyLyS2G266GqY4GxAAkJCfnWavITERoBQPkKx1m5zHWymzHGmLNGvglCVef5vheRcKApsMOnOShP7vJTgImqOjVAkURgr6oeBg6LyHyghXt+LZ9yNYEdBe3vVHiamKLLH2fvXlAFsa4IY8zfWEGXuY4RkSbu6XLACuBD4DcRubGAdQUYB6xT1RF5FPsCuFBEwkSkNE4z0jpgMVBfROqKSATQF5iexzaKhKeJKbp8OseOwZEjwdybMcac/gpqYrpIVe9yT98K/K6q14hIVWAG8HE+63YABgCrRGS5e94TQG0AVR2jqutE5FtgJZAFvKuqqwFE5G5gJhAKvKeqa0784xWepwYRVS4NgORkKFMmmHs0xpjTW0EJIt1n+lLgMwBV3S0FtL+o6gIKvhQWVf038O8A878Bvilo/aLiqUGUKZ+dIOrUKa69G2PM6aegy1wPiMhVIhKPUyP4FkBEwnA6lc8a3j6ImMMA7N5dktEYY0zJK6gGMRh4E6iKcxWS57DZFfg6mIEVN28fRKUUAHbuLMlojDGm5BV0FdPvQPcA82fi9A+cNTyXuZapYAnCGGOggAQhIm/mt1xV7y3acEqOp4kpK+QYlStbgjDGmIKamO4CVgOTgJ0UotP5TOVpYjqWeYzq1S1BGGNMQQmiGtAb6IMz+N6nwGRVPevGOy0V5vS5Hz1+lOrVYdeuEg7IGGNKWL5XManqXvf9Cl1w7oMoD6wVkQHFEl0xioqIAiAlPcVqEMYYQ8E1CABEpBVwI869EDOApcEMqiREu6IBSDnmJIi//oKMDAizcfuMMX9TBXVSPwtciTP8xSfA46qaURyBFbewkDAiwyJJSU+hXjXIyoI9e6B69ZKOzBhjSkZB58f/ArbgDKDXAnjRfQe1AKqqzYMbXvGKjoj21iDAaWayBGGM+bsqKEEU+pkPZ4NoV7TTB+F+rIT1Qxhj/s4KulFuW6D5IhKC0ycRcPmZKjoimtT0VG+tYUdQBxg3xpjTW0HDfZcVkcdFZJSIdBPHPcBm4IbiCbH4eGoQVatCZCRs2lTSERljTMkpqInpI2A/8DNwB85w3QJco6rL81vxTBQVEcXeI3sJCYFzzoE//ijpiIwxpuQU+ExqVW0GICLvAruA2qqaFvTISkB0RDRbD2wFoH592LChZOMxxpiSVNBw38c9E6qaCSSerckBsq9iAidBbNoEmZklHJQxxpSQghJECxE55H6lAM090yJyKL8VRaSWiMwRkbUiskZE7gtQprOIHBSR5e7Xkz7LtorIKvf8JSf38U6Mpw8CnASRng7btxfHno0x5vRT0FVMoaew7QzgIVVdJiLRwFIRmaWqa3OU+1FVr8pjG11UNfkUYjghnquYVJX69Z1xCf/4A+LiiisCY4w5fRRUgzhpqrpLVZe5p1Nw7sauEaz9FYVoVzRZmsXRjKPUr+/Ms45qY8zfVdAShC8RiQPigUUBFrcXkRUiMkNEmvjMV+A7EVkqIoPy2fYgEVkiIkuSkpJOKU7vgH3HUqhWDUqVgo0bT2mTxhhzxgp6ghCRKGAKziNLc/ZbLAPqqGoL4D/A5z7LLlTVVsDlwFAR6Rho+6o6VlUTVDUhNjb2lGKNjnAP2JeeQkgINGwIq1ef0iaNMeaMFdQEISLhOMlhoqpOzblcVQ+paqp7+hsgXEQqud/vcP/cA0wD2gQzVvAf0RWgVSv47TdQDfaejTHm9BO0BCHOqH7jgHWqOiKPMlXFM/qfSBt3PHtFpIy7YxsRKQN0w3myXVD51iDASRDJyZCYGOw9G2PM6SeYTzvoAAwAVomI567rJ4DaAKo6BugFDD12UNoAACAASURBVBGRDOAo0FdVVUSqANPcuSMM+J+qfhvEWIHANQiAZcugVq1g790YY04vQUsQqrqAAp5hraqjgFEB5m/GGV68WOWsQTRvDiEhToK4+urijsYYY0pWsVzFdKbIWYMoXRoaNXIShDHG/N1YgvDhucw1NT3VO691a1i0yDqqjTF/P5YgfHjvg3A3MQF07gxJSbBmTQkFZYwxJcQShI+wkDBKhZXyNjEBXHyx83P27BIKyhhjSogliBx8B+wDqFPHeTaEJQhjzN+NJYgcoiP8EwQ4tYi5cyEjo2RiMsaYkmAJIodoV7RfExPAZZfBwYOwYEEJBWWMMSXAEkQOgWoQl1/uXPL62WclFJQxxpQASxA5REVE5apBlC4NV1wBU6bYE+aMMX8fliByiHZF+90H4dG7N/z1F8ybVwJBGWNMCbAEkUOgJiaAHj0gJgbGjCmBoIwxpgRYgsghOiJ3JzU4Dw+6/XaYOhV27CiBwIwxpphZgsjB08SkAcbWGDIEsrLgP/8pgcCMMaaYWYLIIToiGkU5fPxwrmV160KfPk6C2LOnBIIzxphiZAkiB9/nUgfy9NOQlgYvvFCMQRljTAmwBJGDd8jvAB3VAA0aOH0Rb70Fy5cHLGKMMWeFYD5ytJaIzBGRtSKyRkTuC1Cms4gcFJHl7teTPsu6i8gGEdkoIo8FK86cykeWB2Bnys48y7z8MlSsCHfcAenpxRWZMcYUr2DWIDKAh1S1MdAOGCoijQOU+1FVW7pfzwKISCjwFnA50Bi4MY91i1zHOh0pHV6aiSsn5lkmJgZGj4alS+Hxx4sjKmOMKX5BSxCquktVl7mnU4B1QI1Crt4G2Kiqm1U1HfgEKJaHfpZ1laVX415MXjeZ45nH8yx3/fUwdCiMGAHvv18ckRljTPEqlj4IEYkD4oFFARa3F5EVIjJDRJq459UAtvuUSSSP5CIig0RkiYgsSUpKKpJ4r2lwDQfSDrDgz/xH5xsxArp1c5qapk0rkl0bY8xpI+gJQkSigCnA/ap6KMfiZUAdVW0B/Af4/ES3r6pjVTVBVRNiY2NPPWDg4rrOU4J+3fFrvuUiIpwb59q2hRtugHHjimT3xhhzWghqghCRcJzkMFFVp+ZcrqqHVDXVPf0NEC4ilYAdQC2fojXd84pFuchylHOVY/uh7QWWLVMGZsxwnhlxxx1w333OZbDGGHOmC+ZVTAKMA9ap6og8ylR1l0NE2rjj2QssBuqLSF0RiQD6AtODFWsgtcrVKlSCAChXDr7+2kkOb74JrVrBzz8HOUBjjAmyYNYgOgADgIt9LmO9QkTuEpG73GV6AatFZAXwJtBXHRnA3cBMnM7tSaq6Joix5lKrbC22HyxcggAIC4ORI+Hbb+HQIbjgAujVC37/PYhBGmNMEIUFa8OqugCQAsqMAkblsewb4JsghFYotcrWYvHOxSe83mWXwbp18NprMHy403ndo4dzxVPXrhBityYaY84QdrjKQ61ytUg+kkxaxol3KERHO0NybNwIjzwCCxc6VzvVrQsPPwy//GIPHjLGnP4sQeShcpnKACQdPvlLZ6tWhZdegsREmDgRmjVz+ijat4dKlZx7Kd56CxYvhmPHiipyY4wpGkFrYjrTxZZ2LplNPpJMrXK1CiidP5cLbrrJee3f7/RT/PADzJrlXCYLEB4OzZtDixbOeE+e1znnOMuMMaa4WYLIQ6XSlQBIOlI0N995VKgAN97ovFRh+3anBuF5ffUVvPdedvmwMKhRA2rWzP2qUsUZ9qNiRedlicQYU5QsQeQhtkx2DSJYRKB2bed1/fXZ8w8cgA0bsl9//uk0Uy1bBtOnw9GjgbcXHZ2dLGJinPdRUc7PnC/f+aVKQWRk7pfLZZ3qxvydWYLIg7cGcQp9ECerfHnn7uy2bXMvU3WaqRITnYcW7d3r/0pOdn7u3+88GjU1FVJSnFdGxonHEhHhJIqcySMiAkJDnRpOaKj/dGHnhYRkv4L93pPoRLJ/ltR0ceynIIUpV1RlypRxLsooqX62wn4nZ/I+w8KgYcMgbLfoN3l2qBBZgRAJCWoN4mSIOLWDmJgTW0/V+QdNSfFPGikpzp3fntexY/7vA72OHXNemZnOKyPD+Zmenj3t+ek77fszKyv7lZmZ9/sAT341xuRQpQrs3l3027UEkYfQkFBiSsUUeR9ESRHJPvsvoiGrioWqf/IoKKHk9V41++XZrudncU4Xx34K850WVxmAgwedGmdkZOHKF6WSOMEoiX26XMHZriWIfDSv0pzpG6bzWrfXKBNRpqTD+VsSyW6aMsYUL+uCzMejHR5lV+ou5mydU9KhGGNMsbMEkY+E6gkArE9eX8KRGGNM8bMEkY+YUjFULlPZEoQx5m/JEkQBGlZqaAnCGPO3ZAmiAE1im7BqzyqyNKukQzHGmGJlCaIACdUTOHTsEBv3bSzpUIwxplgF84lytURkjoisFZE1InJfPmXPF5EMEenlMy/T50FDxfo0OV+ejuoGoxrwfz/8X0mFYYwxxS6YNYgM4CFVbQy0A4aKSOOchUQkFHgF+C7HoqOq2tL96hnEOPPVOLYxrlDnLpQXF7xYUmEYY0yxC1qCUNVdqrrMPZ2C8+jQGgGK3gNMAfYEK5ZTERYSRrQruqTDMMaYYlcsfRAiEgfEA4tyzK8BXAv8N8BqkSKyRER+EZFrgh5kPh5q/5B3OiPrJEa8M8aYM1DQE4SIROHUEO5X1UM5Fo8EHlUNeIlQHVVNAG4CRorIOXlsf5A7kSxJSgrOuEmPdniUl7u+DMDu1CCMiGWMMaehoCYIEQnHSQ4TVXVqgCIJwCcishXoBYz21BZUdYf752ZgLk4NJBdVHauqCaqaEBukUehEhGZVmgHw4MwHWbF7RVD2Y4wxp5NgXsUkwDhgnaqOCFRGVeuqapyqxgGTgX+o6uciUkFEXO7tVAI6AGuDFWthNKjYAIDP1n7GFf+7oiRDMcaYYhHM0Vw7AAOAVSKy3D3vCaA2gKqOyWfdRsDbIpKFk8ReVtUSTRDnxGS3cO1O3U1mViahITbEqDHm7BW0BKGqC4BCP1dJVQf6TP8ENAtCWKfkmc7P8NTcp8jSLMKeC2NGvxl0P7d7SYdljDFBYXdSn4AnOz3JskHLvO+nrJ1SgtEYY0xwWYI4QfHV4rmz1Z0A3qfNTVoziRZjWqD2fExjzFnEnih3Esb2GMvBYwdZvGMxADdNuYlMzeTgsYOUjyxfwtEZY0zRsBrESWpVtRVbDmxh0ppJZGomAHuP7C3hqIwxpuhYgjhJF9a+EIA+k/t45+09agnCGHP2sARxkhKqJ3gH8fOwGoQx5mxiCeIkucJctKnRxm+e1SCMMWcTSxCn4KLaF/m9Tz6SzA+bf+CKiVdwPPN4CUVljDFFwxLEKbimof8gs8lHkrnko0uYsXEGa5OcG78HTBvAoC8HlUR4xhhzSixBnILza5xP8j+Tve9/2PKDd7rduHY8OPNBJqycwDvL3sl3O5lZmUGL0RhjTpYliFNUsXRF9Cnl8nMv55fEX7zz0zLSeP2X173vEw8lsi5pXa71f9r+E2HPhbHwz4UAfL7+c+755p4ij/Ng2sEi36Yx5uxmCaKIPNvlWSJCI0ionkDvxr25reVtfsv/NftfNB7dmPu/vZ+MrAwysjL4cduPdHy/IwBzt84F4NpPr2XU4lF0/bArKcdS2HFoB4fTD/tta86WObR9ty1Hjx8tVGxT1k6h/CvlWfDnglP/oCagzKxMhv80nNT01JIO5aRt3r+ZoV8PDdpDsVSV6q9VZ9Svo4KyfVP0LEEUkYTqCRx49AA/3vojk3pPYtzV4/wG8vtgxQcAvLHoDVzPuyj1Qik6ju/ovcmuTEQZv+3N3jKbz9Z+Rs3Xa3Ll/670zt97ZC+XTbiMX3f8yqIdfg/oy9P/zf4/wBkSxFdmVuYpDQ+Smp7KU3Oe4ljGMcA5AFw/6Xomr51c6G1M3zCdeVvnnXQMxW3/0f0M/XporkQwbf00/jnrnwybPayEIgtsQ/KGQietgZ8PZPSS0YQ/F86P234s8lj2p+1nV+ou7plR9DXkM1V6Zjp3fXUXK/9amWvZ5LWT2bx/cwlElc0SRBEqFV6KyLBI7/uPr/+Y969+n+sbXe9XLkuz6BzXmQtqXeCdt2zXMm794lbKucp5590+/XYA5m2b5z2QtxvXjuNZzhVSnmYpX6rKrE2z6D+1P2/9+haqyp8H/wRgyropJB5K9JZtMKoBbd9tm+fneX7+84xePJo/9v5B/6n9OZZxjCfnPMkPm3/wLn92/rNMWDkBgE37NzF13VR6f9bbr0nreOZxJqycwO97f+fPg3/yxfovmLVpFm8veZurP7mazh90zudbzdZpfCfumH5HocoGoqocSDuQa/7h9MO5rjp7eu7T/Lrj11xl31z0JqOXjKb6a9XZtG8TOw7tAPBuN/loMqv3rD6huI5nHmf/0f2A08TYemzrE+qXUlXGLh2b62mHxzOP0/CthlzzSfbFFKnpqWRpFqrKL4m/cDzzONsObANg39F93nJPzX3qhD5DYfj+7RWHpTuX8u+F/y5UWVX1qzkNmz2Maeum5Sp39PhR9h/dz66UXX7zF+9YXKgTnTlb5iDPCDtTdgLw9pK3eXvp27zxyxt+5bI0i96f9abNO20CbabYWIIIovKR5RnYciDNqzQHICI0wrvs2c7PMq7nOO/7j1Z+xPjl4zl4zDmwPt3pab9tbdq/ib1H9rJx30bvvH//9G82JG9gzpY5APSf2p/uE7vTbUI3Jq6ayN0z7ibxUCJHM47Sr1k/dqfu5p2l75CWkca2A9vYtH8Ti3c640mlZaTRYFQDZvwxw7v9YXOGMfSboQyYNoCJqyYyf9t8npv/HJd8dAlb9m/hlYWvAM4fMzj/kB7Ldy/3Tr+56E0GTBvAgzMfpM7IOlzz6TV0m9CNu76+y1tGVRm9eDSjF4/O9T0+P/95bv3iVuZvm8+438b5LXvs+8f4fvP3ef4OfL299G0qvFLB76xs9pbZRL0URf9p/b3zko8k88y8Z2j7blte/zm7H+lw+mE27N0AQEp6Cuf+51xqvl6Tr37/ipRjKQBMWDmBZv9tFjC5eHy25jO/mG/+/GZiXo1BVbnti9tYtmsZa5LWAPDM3Gd47PvHSM9MZ0PyhlzNPxe+dyE9P+nJ4K8G029qPzKzMnl23rMs27WMO750kqnn4okZf8wg+qVoQp8Npd/UfrQf157KwysT90Yck9ZM4ljmMe929x3d55cwcvp+8/fUfr12wNpJXrXSghKE56CZ07GMY96/Mc92PPvNmUhf++k1b7PtdZOu45HvH2HHoR2kZaTlezb+0HcPUeGVCmRkZaCqPP/j81w36Tq/z/LRio8o/WJpYl6NofqI6jw37zm2HdjG2qS1tHm3jfdEJ+lwElv2b/GuN3ntZPpP7U9aRhovL3QeXfzz9p+dZeuc2nZMqRi/z/uPr/8B5L636ljGMe6bcV+u1oBgscH6ikHVqKoAZGRl0KtxLyavnUx8tYBPUAXglUte4ZEOj9A4tjEvLXiJ33b/xqxNs5i0NvuPYljHYTw3/zkavtUQgM96f8bEVRNzbeulBS8B0KtxL2ZvmU3ioUT6Te3H1HXZT4Ads2QMnep04ve9v3PF/65g1OWjaFq5qXe5pynLt0nL94C6ZOcSUtNT2ZW6y295QvUEUo6leGtVX//xdZ6f+a/DfzH0m6EAnF/9fHp+0pNZA2YRVz6OVxe+Skp6irfs0eNHKRVeiizN4pWFr/DKwlfYet9WFu1YxA1NbshzH55mvi37t1CvQj12p+6m64ddAaf57YWLX+CD5R/QOa6zd50Hv3uQB9o/gKrS8K2GAQ9yvyT+QlpGWq55bWq04Z2l73Dw2EEevuBhdqfu5mDaQW6Y7MT4Rd8vqF2uNp+s/gSA1mNbsz/NqUn8tP0nGsc25ul5TwNO7W/jvo30adKHT3o55Tft28TC7dm1yNlbZhP2nPMvHagG4DkZAPh49cdAds2n7+S+KNkHwxV/raDiqxWZ0W8G51Q4h6iIKEYvHs0dre4g+Ugyl350qVNu9wpmbZ5Fl7gudIrrxLhl47j323vZcPcGapat6bd/3++uyvAqzL55Nk0qNwFg/PLx3PrFrfx8+8/Elo4lKiKKKlFV+H7z91zzyTXUq1CPn2//mSPHj1Dr9Vpcds5l1C1flzFLx5D+r3Q2799Mh/c6eA+oGVkZVCpdiT8P/kmdkXW4qM5FzN06l2P/OkZEaATbD26nzbttmDVgFjtTdnovKPlx249+f/tzt86lS90u7D+6n5s/v9nv8zw590menPuk37zbv7id95a/BzjNzgOaD+C+b+9zlsXf7k1su1N3s/fIXm+/4M+JPyPPCAtvW8japLW8vfRt7zbTM9M5kHaAqIgo4kbGkXQkiWnrp+X7t15UgpYgRKQW8CFQBVBgrKq+kUfZ84Gfgb6qOtk97xbgX+4iz6vqB8GKNdiqRVUDnDPtj679iOGXDvdrisqpenR1AHo36U2vxr2o+0Zdxq8Yz687fuXBdg/y2mWvoao8N/857zp5Nb38d8l/AahTrg7Vo6uzM3Un32781q/MkK+H8OWNX3rf3z3j7oDbGjYnu33dd1iRscvGAlA/pj4J1RNYsnMJiYcS8zxjbFeznd8VX+AcED3avOtUq9u9247Dx/076D2fac2eNbzY9UXvvAveu4CdKTvpVKcTVaKqeOdPWDmBXSm7uLP1nd5niU9bP40mlZt4m94AQiWUQV8OYs7WOfx5yJlfuUxl9hzewx3T72BYx2F5fp6kw0ms+Mv/OeU/J/5MxzodGfSVcw/MKwtfIflIsl+Zqz+52u/9b7t/807P3TrXrwnSU3P8dM2nHD5+mC9v/JKvfv8qYDyBPDP3GY4cP5Lnct/kABBXPo6tB7Zy+cTLCQ8Jp2nlpvy2+zciQiP8Doqfr/+c4T8P55l5z7D7od3eWsvapLXULFuTUb+OYtr6aTSv3JyRi0YCUKl0JfYc3sMPW36gfsX6ZGkWL/7o/C7bj2vv3bbnpOXw8cOs2rOK0YtHew+wMzfN9Jb76/BfTF031e9sO/FQInHl41i2axmZmum9COSv1L+oVa4WU9dNZXfqbt745Q2+2/ydd72Zm2byx74/vO8v/vBirqx/JU91KlyTmyc5gHPitO3ANuqUq8O2g9v4JfEXb9PU3TPu9vs/8yT67hO6+50MgdPc9epPrzKz/0zvIwYOpB1g/rb5PD33aab2mRq0UaSD2cSUATykqo2BdsBQEWmcs5CIhAKvAN/5zIsBngLaAm2Ap0SkQhBjDSpPDQIgMiySOuXreN9/fVPus+pOdTp5p0WEgS0Hepssrm98vXe+p1p6XsXzOHjsIJVKVyLpn0nedZvENvFO1ynvJIgNyRsCxvjNH98EjDc6Ijpg+ZwHO4A/9v1B62qtGXr+0FzLfBPivIHzeKDdA97358acS7+p/XKtEyg5gNMc8N7y9/yq2Z7mifnb5nPk+BE+WvERH634iAHTBvDI948Q/3Y8RzOcq77eWvwW1V6rxs3TnDPCO+LvIFMzvdv4dPWnVIiswCuXOE1o434bl28z1thlY3NdMPDJ6k+Ifzu7lhjo+8rPN3984/2dzL1lLne2upN/XvBPAL76/SuSDifxzLxnaF+zPQNbDiQyLJLGsY25p809zOw/M9f2np73NK/+9GrAfVUuUznXvC33bWH2zbMZ3Howx7OOe5NXzjPm4T8P905XfS3778ZzILxnxj3M3jLbmxyaxDZhz8N7iI6IZuO+jfSb2o9yL5fzOyh7zNs2j12pu3im8zPO3+7eDQFrobtTd/vVXj3f0aLERZxf/Xxuj7/dO39d8jp2HNrhbU47fPww+4/up0+TPtQuV5sPVnzA4K8GAzColZPcv/7ja7//D19LBy2lT5M+AZeB88yY9rXac17F85i6fipbDmzJVaZFlRbe6ZzJAeDbTc4J3WUTLgOgf/P+pKSn0Gl8J+ZsncML81/Ic/+nKmgJQlV3qeoy93QKsA6oEaDoPcAUYI/PvMuAWaq6T1X3A7OAM/bZntWiq+W57Ir6V3inv+33Lcn/TKZWuVp+ZYZ1HMadre6keZXmJFRP8M6fNWAWE6+bSO/GvQFoXqU5lUpX8i6/qdlNgFN7iCkVQ/Xo6t4/0Kc6PcWznZ/1lvX9B1h852Je6voSceXjuLmFf7XaY+/RvYy+Ind/QetqrRl1RfZljCHi/In9I+EfdKrTicc6PEZEaAT3tMm+kmVm/5neJpqKpSoCEF81++A6ve/0gDHc++29uebN3TqXt359i5s/v9mvSWDrga25ynr6E7qd0w3A20Z9LPMY1aKrcU6F7OeQj1nq/wh1z2i+Vco4tZUOtTrQsY7T9l3WVTZgvCciJT2Fx394nE51OtGxTkfG9hjLM52fITwkHIDKwyuzP20/Y64aw/tXv8/R/zvKmn+s4c3L36RLXBfvdnY+6N+uf27Mud4arecEwrdv7JYWt/BZ788A6FK3C2OuGuP9rHlpWyP7QgfPwW536u5cHf8fXvMhywYvQ0Q4N+ZcNu7byOS1k0nPTHf25xP38sHLvWfFbWq0oXKZyqzes5qlu7L7uTy/t20HtuXqv7hnxj3sSNlB+cjytK+ZXSu5bMJl1Hy9Jpv2bQKcWltKegptarQhtnSsX0f/4xc97k2evs27vlpVa8XH13/s158I0LdpX+90o0qNaFezHUt2LgFgQPMB3mUz+8+kdrnaAbftkfMKp1tb3ur3fv6f8/Nd/1QUSye1iMQB8cCiHPNrANcC/82xSg1gu8/7RAInF0RkkIgsEZElSUlJgYqUOM9BpCDNqjSjYumKueaHhoQytsdYVty1wu+fuVW1VtzU7CbOq3geQK7RZTvW6cg7Pd5h4W1O9dXT3hwWEsa9be/1O3PcdtC5kuWDaz6gZtmaPHbhY2y5bwsX1704YKy1y9Xmtvjb/P6pAS6pdwmAt3nki75fMKjVIB5o/wBzB87lpUucPhFP+/SznZ+lXoV63lhGdh9J5pOZzBowi1AJZVKvSfRo0IPP+3zOnoezzyG61u3qnR5zZfbBe+62uaxPXu9937NBT+90vQr1An6Wy+s7zSieq8PASVQdanfgze5vUiO6hvef26NllZaAk0wPPnaQBbctYFqfadwefzvT+kzzO0Dkx3OWWtZVlpGXOWfZt8ffTmzpWADeu/o9RJxHu5cKL8XE67L7mbqd0817AYSv8NBwoiOiSaie4FcbBP/agqet3bejd1jHYfRq3MtvnRrR/v96vicWd8TfwfhrxnvfT+o9iaiIKFYnrSb237He+WXCyzCgxQDv3++5Med6m308Xr/sdVYNWcVD7R+iWZVmTO49mRZVWtChVgeqlKniraWdX/18wDnwAvT6rBdT1gV+/G+0K5oWVVvkmr9gu9P27/lbqVOujt/JVb0K9ahTrg67H9pNhcgK3qFzfC9d/+k2p1lURLzfmSf5tqjSwjtWW5PYJrSr0c673vhrxnu/h2aVm1GhlNM4EhURxUfXfsTW+7bmivfSepd6p+OrxvNez+ymrF93/Op9eFlRC3qCEJEonBrC/ap6KMfikcCjqj6XKJwgVR2rqgmqmhAbG1vwCiUgPDScm5rdxJQb8n+Gte8f6Im4/NzLqR9Tn2e7POs3v0JkBe5odQc1yjr/4J7LbTffu5mYUjGUiyznV76sq2yuGoPn7NDzmFWPrnW74gpzMbP/TA4/cZg74u/g/y76P+pWqAs4taEt923hqvOu4u0eb+fqsAwPDSfrySz+1dHpZvKcLVYuU5kQCaFi6YpkPJlB7yZO7ejqhlcTWyb79zujX/bVVhfVuYit923lxYtfZG3SWuZum+tdNiRhiLd5y3NmnFNURBStqrXym1exdEVCJIR72t7DlfWd+1A8/9QNKzXk1UtfZfWQ1dQqV8tbY4gpFcO7Pd/l4roX82iHRwPuC5za27yB8xjXcxxXnXcVAMMvHc597e7jm5u+4Y3ub7ByyErW/mNtrqTm27/ybT//viRfSf9M4qfbfkJEvAdScL5fz703nvme90DAs9lS4aX83nc/tzsjuo1g/dD1vNPzHRpWauhdVrd8XapFVWPCygneK/Iguybp0aBiA2+TnyeJNajUgKaVmzK823BCJISu9bqy/K7lRLuivYmtUulKvHyJcyXQLS1u8dtmj/N68FSnp/yaRcu6ytK6Wmu/kwhw+khaVm3pfV+nfHaCuK/tfWy6dxMigojwUteXKOsqy+DWg73/Q7e0uIX2tdr77UefUt68/E3ASRAz+89kUq9JXNPwGu8J04PtHiREQpg3cB4Pt3+YqlFVqRDpJIi48nH0b97frwnaw/fEsXxkeRrFNvJb3ubdNoW+cfZEBPUqJhEJx0kOE1V1aoAiCcAn7jOkSsAVIpIB7AA6+5SrCcwNZqzB5nvmlxff2sGJiC0Ty+/3/J5rvufMxKN3k95kNs70/rP63nMBTkdtTjXK1uCJC5+gR4MefmNKec6UwkPDCQ8N552e/uNNRbuiiXYF7r/w8JwZQ3aCyK/zHmDBrQtYm7SW8NBw77yKpSpSJaoK1zW6jufmP8fm/ZsZ0HwAI7uPJKZUDD/e6tz01apaKyZeN5GN+zZ6r/Lx7O+K+lf49SN4mroAOsV1YuyysaRnprPt/m2Uc5WjVHgp7xU4gfgeaKf3nU7fKX05cvwIswbM8tayOtbpyLGMYzxywSP0aNADcGoz4Nw4mfPsH/xrAL7fX06usOza5MohK2n+3+asS15HbOlYb42hWZVm9G/en7vPv5uPVn7Ex6s/9vtePa6sfyXjl49nSMIQ3vvtPZpWbsr5Nc4PuN/w0HCqRVfL1aeQs0ble3D9/e7fST6SnO/v3vO5O8d15uK6F6NP5b6UdtvBbUy/cTqTLSNQiwAACv1JREFU1kxiXbIzrE2psFKICIMTBvtdVn1nqzsZcdkIOr7fkf1p+2lQsYG3Bp7zZGZwwmAGJzj9Eh8sd66VySvWXo17seHuDd5avecEp0XVFiwdtNSblNrVbEe7mk6twpMg/P7m6nRi3rbs+yp8L28WEW9Sji0dS9KRJG5teWuuRF4UgnkVkwDjgHWqOiJQGVWt61N+PPCVqn7u7qR+0adjuhvweLBiLWlXnXfVCV2RUliePzxfvmdynj+oalHVqF2utl811tcLXZ1OsGWDltFuXDvSM9OL/KqJ0VeM5p4Z99C6Wut8y3Wo3YEOtTv4zfMkwgaVGvDm5W9y55d3EhYS5u3E9+23uanZTRw9ftSbIDyDLd4efztPzX0KV6iL/2/v/mOrKu84jr8/FAQ2naX8shNcMXTMKsiPZv6gGZ2bi0MxGmkYcYMpiY4sm2bLpDLH2Fw0mmVzOOZ04jaZ0WmcjvgHyIDohkZs+a2MiQv4CwUmSHCOCP3uj/Ocy+ntuW1pe3vh3u8ruek5zzn39vnent7vfZ7nnOccPnq4VXwNNQ3cu+5eRpWP6rC/OFOnxHs/bcw0Ppz/Ifv+u69NK7F/3/7cdeldnXpN6Hx3ZVLfPn0zH/zVFdWZFsPggYNZevVSAC4YcUGrsaOk6TXTeed771B5WiW/ubztuBPAkiuXZK5VqK6o5vldx/rF59fN58f1rc8Cij8coXNfJuKkNm5Y6y61hpoGRpWPonl3c6bVlkyO8fgGRN2nK15fweKpizN/3/U3rs9sjweu22vJzzhvBs27m1kwZUHOfeLkkC27lRqbUjWF8pfKW52csvzry2l4oiHzuXD3l++mYkAF7xyKxlrKB5Sz8caNjK4YzbLty7j6nKtz1qc78tmCmAx8A9giKb5qaj5wFoCZ/TbXE83sfUm3A3HH2k/NLPdVOye5XIOwXbVm9hqWblra4TeK+CKgscPHpp75km1C5QRmjZvFgxse7PAf+nhN+vQkXpjzQsc7JtQMreHVva+2anldN/463jr4VquBwGzx+zKpclJmipMzP3Um79/yPvc13ZeZmiTWr6xfpr+5syTx+PTHW3W/dLULMSn+YItbcJ0Vn1V03rDzMh+22S3M9rR3ogXA9ROOzT1WX1XPkg1LmFg5kSuqr6CxrrFN67hiYAV3fulOJo+cnP1SqeJv7NnjCY83tB08To7FJa9PmXX+rJwnXST3zR7Ly65H3I3UU+qr6tk/b3+b3xPX49FrHmXUoFHcP+3+VvvE78XMsTN7tD5JeUsQZvYPIHcbuO3+38xafwh4KH3v4tJeV0FX1FfVt7rYK5e6s+qYWzuXxrrGTr923EQeO2xsV6vXY9Zev7bNlAdlfcpYWL+ww+e+cfMbbT4gBw0clPmnzL5iuSt/o7h7oSdJYvWs1YwZMua4nhefP3/usHMzLYi0FmZPiAdnrxpzFT+akntuquM57m77wm2cM/Qcpn12Wof7Xjzy4kx3YfYFjO1prGtk7ZtrM12AhTa1eipPbnuy1Rl9vc7MiuYxadIkc/nV0tJiW9/bWuhq5M3DGx82FmK3P3d7oavSoxasXmAsxFpaWuyMn59hLMQO/u9g3n7frgO77MjRI3l7/fYcPnLY7nnxHmMhdsfzdxSkDj2hpaXFDnx0IO+/B2iyHJ+psm7M5nmiqa2ttaampo53dC6HFmthyfolzB4/u8snDZzotu3dxrLty5hXl/tMq2Kw5b0t1AytoaxP25Mv3DGSms2sNnWbJwjnnCtd7SUIn83VOedcKk8QzjnnUnmCcM45l8oThHPOuVSeIJxzzqXyBOGccy6VJwjnnHOpPEE455xLVVQXyknaC+zqwlOHAMd3T8iTn8dcGjzm0tCdmD9jZqk30ymqBNFVkppyXUlYrDzm0uAxl4Z8xexdTM4551J5gnDOOZfKE0TkgUJXoAA85tLgMZeGvMTsYxDOOedSeQvCOedcKk8QzjnnUpV8gpB0maTtknZI6vxNck9wkh6StEfS1kRZhaSVkl4LPweFcklaFN6DzZImFq7mXSNppKQ1kl6V9Iqkm0J50cYMIGmApHWSNoW4fxLKR0l6KcT3Z0mnhPL+YX1H2F5VyPp3laQySRskPRPWizpeAEk7JW2RtFFSUyjL6/Fd0glCUhmwGPgqUAPMlFRT2Fr1mD8Al2WVNQKrzKwaWBXWIYq/OjxuAO7rpTr2pCPA982sBrgQ+Hb4WxZzzACHgUvM7HxgPHCZpAuBu4BfmtloYD8wJ+w/B9gfyn8Z9jsZ3QRsS6wXe7yxL5rZ+MQ1D/k9vnPdrLoUHsBFwIrE+q3ArYWuVw/GVwVsTaxvByrDciWwPSzfD8xM2+9kfQB/BS4tsZg/AawHLiC6qrZvKM8c58AK4KKw3Dfsp0LX/TjjHBE+DC8BngFUzPEm4t4JDMkqy+vxXdItCOBM4M3E+luhrFgNN7PdYfldYHhYLqr3IXQjTABeogRiDt0tG4E9wErgdeCAmR0JuyRjy8Qdtn8ADO7dGnfbPcAtQEtYH0xxxxsz4FlJzZJuCGV5Pb77drWm7uRmZiap6M5xlnQq8CRws5kdlJTZVqwxm9lRYLykcuAp4HMFrlLeSLoC2GNmzZLqC12fXlZnZm9LGgaslPTP5MZ8HN+l3oJ4GxiZWB8RyorVe5IqAcLPPaG8KN4HSf2IksMjZvaXUFzUMSeZ2QFgDVEXS7mk+AtgMrZM3GH76cB/ermq3TEZuFLSTuAxom6mX1G88WaY2dvh5x6iLwKfJ8/Hd6kniJeB6nAGxCnA14BlBa5TPi0DZofl2UT99HH5rHDmw4XAB4lm60lBUVNhCbDNzH6R2FS0MQNIGhpaDkgaSDTuso0oUUwPu2XHHb8f04HVFjqpTwZmdquZjTCzKqL/19Vmdi1FGm9M0iclnRYvA18BtpLv47vQAy+FfgBTgX8R9dv+sND16cG4HgV2Ax8T9T/OIep7XQW8BvwNqAj7iuhsrteBLUBtoevfhXjriPpoNwMbw2NqMccc4hgHbAhxbwUWhPKzgXXADuAJoH8oHxDWd4TtZxc6hm7EXg88Uwrxhvg2hccr8WdVvo9vn2rDOedcqlLvYnLOOZeDJwjnnHOpPEE455xL5QnCOedcKk8QzjnnUnmCcK4Dko6GGTTjR4/N+iupSokZd507kfhUG8517CMzG1/oSjjX27wF4VwXhfn57w5z9K+TNDqUV0laHebhXyXprFA+XNJT4d4NmyRdHF6qTNLvwv0cng1XRCPpu4rub7FZ0mMFCtOVME8QznVsYFYX04zEtg/MbCzwa6JZRgHuBf5oZuOAR4BFoXwR8JxF926YSHRFLERz9i82s3OBA8A1obwRmBBe51v5Cs65XPxKauc6IOmQmZ2aUr6T6GY9/w4TBb5rZoMl7SOae//jUL7bzIZI2guMMLPDideoAlZadMMXJM0D+pnZzyQtBw4BTwNPm9mhPIfqXCvegnCueyzH8vE4nFg+yrGxwcuJ5tOZCLycmK3UuV7hCcK57pmR+PliWH6BaKZRgGuBv4flVcBcyNzk5/RcLyqpDzDSzNYA84imqW7TinEun/wbiXMdGxju2BZbbmbxqa6DJG0magXMDGXfAX4v6QfAXuC6UH4T8ICkOUQthblEM+6mKQP+FJKIgEUW3e/BuV7jYxDOdVEYg6g1s32Frotz+eBdTM4551J5C8I551wqb0E455xL5QnCOedcKk8QzjnnUnmCcM45l8oThHPOuVT/B/8C+5LitfWAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train = history.history['loss']\n",
        "loss_val = history.history['val_loss']\n",
        "epochs = range(1,len(loss_train)+1)\n",
        "plt.plot(epochs, loss_train, 'g', label='Training rmse')\n",
        "plt.plot(epochs, loss_val, 'b', label='validation rmse')\n",
        "plt.title('Training and Validation MAPE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Ny-wGn2xLH",
        "outputId": "54caa15e-e5f3-4c55-fbc6-22169edd9a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            " CNN Predicted values  [ 3.04  3.74  2.69  0.67  2.1   3.65  0.69 -0.57 -0.53  2.86  3.53  0.58\n",
            "  3.53  1.05  0.    2.96 -0.61  3.49  0.52  0.42  0.04  0.57  0.92  1.52\n",
            "  3.67  3.31  3.59  3.28  0.21  2.57]\n",
            "Real values\t  [1.5, 5.5, 1.5, 0.5, 1.5, 5.5, 0.1, 0.08, 1, 3.2, 5.5, 0.5, 3.5, 0.1, 0.5, 1.5, 0.09, 5.5, 0.5, 0.1, 1, 0.1, 0.4, 1, 6.6, 1.8, 3.5, 1.8, 0.1, 2]\n",
            " The Mean absolute percentage Error is -> 225.281251506089 %\n"
          ]
        }
      ],
      "source": [
        "# Predict test set labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print( \" CNN Predicted values \", np.round( Y_pred[:30], 2 ) )\n",
        "\n",
        "y_pred =y_pred[:, 0] \n",
        "print( \"Real values\t \", Y_test[:30] )\n",
        "\n",
        "#print( \" Coefficients of the LR \",  model.coef_, model.intercept_ )\n",
        "\n",
        "\n",
        "\n",
        "mse = mean_absolute_percentage_error(Y_test, y_pred) *100 ##( |ypred - ytrue|/ytrue * 100)\n",
        "\n",
        "print(' The Mean absolute percentage Error is ->', mse, '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "mYyJV8xS2xLH"
      },
      "outputs": [],
      "source": [
        "NN_model.save('./saved_models/baseline_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s848pgR62xLH",
        "outputId": "4a1af0a9-1d3a-457c-e24d-1d17539f8301"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33480"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "import os\n",
        "os.path.getsize('./saved_models/baseline_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LiQ15FZ6RVo",
        "outputId": "bddcfddb-1ec1-4e7c-8b17-032aaeae5225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.21.6)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (0.1.7)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing with the `--upgrade` flag ensures you'll get the latest version.\n",
        "!pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBTfazcd2xLH"
      },
      "source": [
        "PRUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "3jMFN2Hz2xLH"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "mDivYvi72xLI"
      },
      "outputs": [],
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "BhwhPLbz2xLI"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "end_step = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "yz9wGd0Y2xLI"
      },
      "outputs": [],
      "source": [
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=end_step)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "4BioniUD2xLI"
      },
      "outputs": [],
      "source": [
        "pruned_model = prune_low_magnitude(NN_model,**pruning_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "3b7wn3YM2xLI"
      },
      "outputs": [],
      "source": [
        "pruned_model.compile(optimizer='rmsprop',loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "QafG5KUY2xLI"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "log_dir = tempfile.mkdtemp()\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir=log_dir)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nIEQX6D2xLI",
        "outputId": "012889d2-baed-490f-81a4-9731d4601238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            " 1/40 [..............................] - ETA: 1:00 - loss: 1.6799"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 0.0062s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 2s 7ms/step - loss: 2.1650 - val_loss: 1.6170\n",
            "Epoch 2/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 1.5164 - val_loss: 1.2997\n",
            "Epoch 3/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.2597 - val_loss: 2.9350\n",
            "Epoch 4/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9947 - val_loss: 2.9372\n",
            "Epoch 5/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9954 - val_loss: 2.9359\n",
            "Epoch 6/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9932 - val_loss: 2.9331\n",
            "Epoch 7/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9915 - val_loss: 2.9325\n",
            "Epoch 8/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9898 - val_loss: 2.9358\n",
            "Epoch 9/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9880 - val_loss: 2.9361\n",
            "Epoch 10/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9873 - val_loss: 2.9376\n",
            "Epoch 11/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9856 - val_loss: 2.9323\n",
            "Epoch 12/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9846 - val_loss: 2.9344\n",
            "Epoch 13/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9854 - val_loss: 2.9330\n",
            "Epoch 14/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9828 - val_loss: 2.9363\n",
            "Epoch 15/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9835 - val_loss: 2.9374\n",
            "Epoch 16/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9821 - val_loss: 2.9321\n",
            "Epoch 17/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9820 - val_loss: 2.9329\n",
            "Epoch 18/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9799 - val_loss: 2.9316\n",
            "Epoch 19/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9795 - val_loss: 2.9362\n",
            "Epoch 20/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9798 - val_loss: 2.9342\n",
            "Epoch 21/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9799 - val_loss: 2.9328\n",
            "Epoch 22/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9785 - val_loss: 2.9327\n",
            "Epoch 23/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9764 - val_loss: 2.9352\n",
            "Epoch 24/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9763 - val_loss: 2.9389\n",
            "Epoch 25/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9759 - val_loss: 2.9312\n",
            "Epoch 26/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9773 - val_loss: 2.9364\n",
            "Epoch 27/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9759 - val_loss: 2.9320\n",
            "Epoch 28/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9771 - val_loss: 2.9316\n",
            "Epoch 29/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9762 - val_loss: 2.9314\n",
            "Epoch 30/30\n",
            "40/40 [==============================] - 0s 3ms/step - loss: 2.9775 - val_loss: 2.9328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f059bfdda10>"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ],
      "source": [
        "pruned_model.fit(x=X_train,y=np.array(Y_train),epochs=epochs,validation_data=(X_test,np.array(Y_test)),callbacks=callbacks) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFZrsmBR2xLI",
        "outputId": "0075574d-0012-4791-d276-92f734e5e32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions=pruned_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "NkTZtbDh2xLI"
      },
      "outputs": [],
      "source": [
        "pruned_rmse = np.sqrt(mean_squared_error(Y_test,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQXdaU5I-u9z",
        "outputId": "641003b5-e43b-4449-dc59-bbb08459017d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pruned_rmse ->  1.712541272256738\n"
          ]
        }
      ],
      "source": [
        "print(\"pruned_rmse -> \", pruned_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yS3y0Ps2xLI",
        "outputId": "3cedbdff-e11c-4c75-98bf-99d6ca47be82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
        "model_for_export.save('./saved_models/pruned_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "id": "vRmElrfg2xLI"
      },
      "outputs": [],
      "source": [
        "pruned_model_size = os.path.getsize('./saved_models/pruned_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "id": "1PhuylRF2xLI"
      },
      "outputs": [],
      "source": [
        "baseline_model_size = os.path.getsize('./saved_models/baseline_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sJ5ccJq2xLI",
        "outputId": "e38087b3-7b7b-4e73-c913-ad6a8ed101df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18264\n"
          ]
        }
      ],
      "source": [
        "print(pruned_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xjNkIXW2xLI",
        "outputId": "ddf52b8e-1fd0-45c7-ca44-99b7c104afe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33480\n"
          ]
        }
      ],
      "source": [
        "print(baseline_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbO8CRmN2xLI",
        "outputId": "d42bb4f2-e47f-4632-d83e-cb68bffe086c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.712541272256738\n"
          ]
        }
      ],
      "source": [
        "print(pruned_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YItkZZq2xLI",
        "outputId": "c351f01a-34f8-4197-c63d-367b8edb023c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6212357765676298\n"
          ]
        }
      ],
      "source": [
        "print(base_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLbHBNeU2xLJ",
        "outputId": "95dcca48-50d6-4702-b39b-2fc45eb28902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned/Striped Prediction time: 6.105374097824097\n",
            "Baseline Prediction time: 6.060596466064453\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "\n",
        "for _ in range(100):\n",
        "    predictions = model_for_export.predict(X_test, verbose=0)\n",
        "end = time.time()\n",
        "pruned_pred_time = end-start\n",
        "print(f'Pruned/Striped Prediction time: {end-start}')\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    predictions = NN_model.predict(X_test, verbose=0)\n",
        "end = time.time()\n",
        "baseline_pred_time = end-start\n",
        "print(f'Baseline Prediction time: {end-start}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1E48cnv2xLJ"
      },
      "source": [
        "QUANTIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "UZwen5GE2xLJ"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "with open('./saved_models/pruned_quantized_model.tflite', 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "Lgls1k3r2xLJ"
      },
      "outputs": [],
      "source": [
        "pruned_quantized_model_size = os.path.getsize('./saved_models/pruned_quantized_model.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gapp6vI62xLJ",
        "outputId": "d88191db-cbe6-4dee-a50c-dd32e50e20c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33480\n"
          ]
        }
      ],
      "source": [
        "print(baseline_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnerQL_w2xLJ",
        "outputId": "261d5017-a27d-4c59-d362-925305af62ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18264\n"
          ]
        }
      ],
      "source": [
        "print(pruned_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6gUv8S2xLJ",
        "outputId": "3f1b7dab-6c62-4097-dd47-de73b1f6ecff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2320\n"
          ]
        }
      ],
      "source": [
        "print(pruned_quantized_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "eHIHrbyq2xLJ"
      },
      "outputs": [],
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "# How to predict with tfLite mode [Unzipping]\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_shape = input_details[0]['shape']\n",
        "predictions = []\n",
        "for i in range(len(X_test)):\n",
        "    input_data = X_test[i]\n",
        "    input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(output_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th4JJSTh2xLJ",
        "outputId": "fea144fa-3de0-429c-c5ae-fed5fd2309da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.7125412567078007\n"
          ]
        }
      ],
      "source": [
        "predictions = np.array(predictions).reshape(-1, 1)\n",
        "pruned_quantized_rmse = np.sqrt(mean_squared_error(Y_test, predictions))\n",
        "print(f'RMSE: {np.sqrt(mean_squared_error(Y_test, predictions))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q7nT7OL2xLJ",
        "outputId": "8fce27d1-ae72-477d-e90f-08d450aba417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.597258753124207\n"
          ]
        }
      ],
      "source": [
        "print(base_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e10KKYc2xLJ",
        "outputId": "438f35b8-6549-4156-8fd7-99e3e7506e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5447936887265623\n"
          ]
        }
      ],
      "source": [
        "print(pruned_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmxyuwg2xLJ",
        "outputId": "c100ae36-326c-4fdd-8637-38f8f1d5b3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5450361501845076\n"
          ]
        }
      ],
      "source": [
        "print(pruned_quantized_rmse)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ccfd5eb523b484a979b6e02c6ef18cb5a7b8f195775124f8ffab4df81f42c205"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}